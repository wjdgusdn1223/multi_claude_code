#!/usr/bin/env python3
"""
Smart File Discovery System for Multi-Agent Claude Code
ì§€ëŠ¥ì  íŒŒì¼ íƒìƒ‰ ë° ë©”íƒ€ë°ì´í„° ì‹œìŠ¤í…œ
"""

import os
import json
import yaml
import hashlib
import mimetypes
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any, Union, Tuple
from enum import Enum
from dataclasses import dataclass, asdict
import sqlite3
import threading
import time
import fnmatch
import re

class FileType(Enum):
    """íŒŒì¼ íƒ€ì…"""
    SOURCE_CODE = "source_code"
    DOCUMENTATION = "documentation"
    CONFIGURATION = "configuration"
    DATA = "data"
    TEMPLATE = "template"
    SPECIFICATION = "specification"
    TEST = "test"
    BUILD_ARTIFACT = "build_artifact"
    MEDIA = "media"
    UNKNOWN = "unknown"

class FileStatus(Enum):
    """íŒŒì¼ ìƒíƒœ"""
    ACTIVE = "active"
    DEPRECATED = "deprecated"
    ARCHIVED = "archived"
    DELETED = "deleted"
    DRAFT = "draft"
    REVIEW = "review"

class AccessPattern(Enum):
    """ì ‘ê·¼ íŒ¨í„´"""
    FREQUENT = "frequent"
    OCCASIONAL = "occasional"
    RARE = "rare"
    NEVER_ACCESSED = "never_accessed"

@dataclass
class FileMetadata:
    """íŒŒì¼ ë©”íƒ€ë°ì´í„°"""
    file_path: str
    file_name: str
    file_type: FileType
    file_status: FileStatus
    size_bytes: int
    created_at: datetime
    modified_at: datetime
    last_accessed: Optional[datetime]
    access_count: int
    access_pattern: AccessPattern
    content_hash: str
    mime_type: str
    encoding: Optional[str]
    language: Optional[str]
    framework: Optional[str]
    dependencies: List[str]
    tags: List[str]
    purpose: Optional[str]
    role_ownership: Optional[str]
    project_phase: Optional[str]
    quality_score: float
    complexity_score: float
    maintainability_score: float

@dataclass
class FileRelationship:
    """íŒŒì¼ ê°„ ê´€ê³„"""
    from_file: str
    to_file: str
    relationship_type: str  # "imports", "includes", "references", "generates", "tests"
    strength: float
    discovered_at: datetime

@dataclass
class SearchQuery:
    """ê²€ìƒ‰ ì¿¼ë¦¬"""
    keywords: Optional[List[str]] = None
    file_types: Optional[List[FileType]] = None
    roles: Optional[List[str]] = None
    project_phases: Optional[List[str]] = None
    tags: Optional[List[str]] = None
    modified_since: Optional[datetime] = None
    size_range: Optional[Tuple[int, int]] = None
    quality_threshold: Optional[float] = None
    access_pattern: Optional[AccessPattern] = None
    content_regex: Optional[str] = None
    exclude_patterns: Optional[List[str]] = None
    limit: Optional[int] = None

class SmartFileDiscoverySystem:
    """ì§€ëŠ¥ì  íŒŒì¼ ë°œê²¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.discovery_dir = self.project_root / "file_discovery"
        self.metadata_db = self.discovery_dir / "file_metadata.db"
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.discovery_dir.mkdir(exist_ok=True)
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        self._init_database()
        
        # íŒŒì¼ íƒì§€ ê·œì¹™
        self.detection_rules = self._initialize_detection_rules()
        
        # ìºì‹œ
        self.metadata_cache: Dict[str, FileMetadata] = {}
        self.cache_lock = threading.Lock()
        
        # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ìº” ì„¤ì •
        self.scan_active = True
        self.last_scan_time = datetime.min
        
        # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œ
        self.monitor_thread = threading.Thread(target=self._monitor_file_changes, daemon=True)
        self.monitor_thread.start()
        
        print("ğŸ” Smart File Discovery System ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        with sqlite3.connect(self.metadata_db) as conn:
            conn.execute('''
                CREATE TABLE IF NOT EXISTS file_metadata (
                    file_path TEXT PRIMARY KEY,
                    file_name TEXT NOT NULL,
                    file_type TEXT NOT NULL,
                    file_status TEXT NOT NULL,
                    size_bytes INTEGER NOT NULL,
                    created_at TEXT NOT NULL,
                    modified_at TEXT NOT NULL,
                    last_accessed TEXT,
                    access_count INTEGER DEFAULT 0,
                    access_pattern TEXT NOT NULL,
                    content_hash TEXT NOT NULL,
                    mime_type TEXT NOT NULL,
                    encoding TEXT,
                    language TEXT,
                    framework TEXT,
                    dependencies TEXT,  -- JSON array
                    tags TEXT,          -- JSON array
                    purpose TEXT,
                    role_ownership TEXT,
                    project_phase TEXT,
                    quality_score REAL DEFAULT 0.5,
                    complexity_score REAL DEFAULT 0.5,
                    maintainability_score REAL DEFAULT 0.5,
                    indexed_at TEXT NOT NULL,
                    updated_at TEXT NOT NULL
                )
            ''')
            
            conn.execute('''
                CREATE TABLE IF NOT EXISTS file_relationships (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    from_file TEXT NOT NULL,
                    to_file TEXT NOT NULL,
                    relationship_type TEXT NOT NULL,
                    strength REAL NOT NULL,
                    discovered_at TEXT NOT NULL,
                    FOREIGN KEY (from_file) REFERENCES file_metadata (file_path),
                    FOREIGN KEY (to_file) REFERENCES file_metadata (file_path)
                )
            ''')
            
            conn.execute('''
                CREATE TABLE IF NOT EXISTS access_log (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    file_path TEXT NOT NULL,
                    accessed_by TEXT NOT NULL,
                    access_type TEXT NOT NULL,  -- read, write, execute
                    accessed_at TEXT NOT NULL,
                    FOREIGN KEY (file_path) REFERENCES file_metadata (file_path)
                )
            ''')
            
            # ì¸ë±ìŠ¤ ìƒì„±
            conn.execute('CREATE INDEX IF NOT EXISTS idx_file_type ON file_metadata (file_type)')
            conn.execute('CREATE INDEX IF NOT EXISTS idx_file_status ON file_metadata (file_status)')
            conn.execute('CREATE INDEX IF NOT EXISTS idx_modified_at ON file_metadata (modified_at)')
            conn.execute('CREATE INDEX IF NOT EXISTS idx_role_ownership ON file_metadata (role_ownership)')
            conn.execute('CREATE INDEX IF NOT EXISTS idx_project_phase ON file_metadata (project_phase)')
            conn.execute('CREATE INDEX IF NOT EXISTS idx_quality_score ON file_metadata (quality_score)')
    
    def _initialize_detection_rules(self) -> Dict[str, Any]:
        """íŒŒì¼ íƒì§€ ê·œì¹™ ì´ˆê¸°í™”"""
        return {
            'file_type_patterns': {
                FileType.SOURCE_CODE: [
                    '*.py', '*.js', '*.ts', '*.tsx', '*.jsx', '*.java', '*.cpp', '*.c', '*.cs', 
                    '*.go', '*.rs', '*.rb', '*.php', '*.swift', '*.kt', '*.scala', '*.clj'
                ],
                FileType.DOCUMENTATION: [
                    '*.md', '*.rst', '*.txt', '*.doc', '*.docx', '*.pdf', 'README*', 'CHANGELOG*',
                    'CONTRIBUTING*', 'LICENSE*', '*.wiki'
                ],
                FileType.CONFIGURATION: [
                    '*.json', '*.yaml', '*.yml', '*.toml', '*.ini', '*.cfg', '*.conf',
                    '*.properties', '.env*', 'Dockerfile*', '*.dockerfile'
                ],
                FileType.DATA: [
                    '*.csv', '*.json', '*.xml', '*.sql', '*.db', '*.sqlite', '*.sqlite3',
                    '*.parquet', '*.avro', '*.jsonl'
                ],
                FileType.TEMPLATE: [
                    '*.jinja', '*.j2', '*.template', '*.tmpl', '*.tpl', '*.mustache', '*.hbs'
                ],
                FileType.SPECIFICATION: [
                    '*spec.md', '*specification.md', '*requirements.md', '*.proto', '*.avsc',
                    '*.openapi', '*.swagger', '*-spec.json', '*-specification.json'
                ],
                FileType.TEST: [
                    '*test*.py', '*_test.py', 'test_*.py', '*test*.js', '*test*.ts',
                    '*spec*.py', '*spec*.js', '*spec*.ts', '*.test.*'
                ],
                FileType.BUILD_ARTIFACT: [
                    '*.jar', '*.war', '*.ear', '*.tar', '*.zip', '*.gz', '*.bz2',
                    'dist/*', 'build/*', 'target/*', 'node_modules/*'
                ],
                FileType.MEDIA: [
                    '*.png', '*.jpg', '*.jpeg', '*.gif', '*.svg', '*.ico', '*.webp',
                    '*.mp4', '*.avi', '*.mov', '*.mp3', '*.wav'
                ]
            },
            'language_detection': {
                '.py': 'Python',
                '.js': 'JavaScript',
                '.ts': 'TypeScript',
                '.tsx': 'TypeScript',
                '.jsx': 'JavaScript',
                '.java': 'Java',
                '.cpp': 'C++',
                '.c': 'C',
                '.cs': 'C#',
                '.go': 'Go',
                '.rs': 'Rust',
                '.rb': 'Ruby',
                '.php': 'PHP',
                '.swift': 'Swift',
                '.kt': 'Kotlin',
                '.scala': 'Scala',
                '.clj': 'Clojure'
            },
            'framework_detection': {
                'package.json': ['React', 'Vue', 'Angular', 'Express', 'Next.js'],
                'requirements.txt': ['Django', 'Flask', 'FastAPI', 'Pandas'],
                'Gemfile': ['Rails', 'Sinatra'],
                'composer.json': ['Laravel', 'Symfony'],
                'pom.xml': ['Spring', 'Maven'],
                'build.gradle': ['Spring Boot', 'Android']
            },
            'exclude_patterns': [
                '*.pyc', '__pycache__/*', '.git/*', '.svn/*', '.hg/*',
                'node_modules/*', 'venv/*', '.venv/*', 'env/*',
                '*.log', '*.tmp', '*.cache', '.DS_Store', 'Thumbs.db'
            ],
            'quality_indicators': {
                'high_quality': ['test', 'spec', 'documentation', 'readme'],
                'complexity_keywords': ['if', 'for', 'while', 'switch', 'case', 'try', 'catch'],
                'maintainability_keywords': ['class', 'function', 'def', 'interface', 'abstract']
            }
        }
    
    def scan_project_files(self, force_rescan: bool = False) -> Dict[str, Any]:
        """í”„ë¡œì íŠ¸ íŒŒì¼ ì „ì²´ ìŠ¤ìº”"""
        
        start_time = datetime.now()
        
        # ì´ë¯¸ ìµœê·¼ì— ìŠ¤ìº”í–ˆê³  ê°•ì œ ì¬ìŠ¤ìº”ì´ ì•„ë‹Œ ê²½ìš° ìŠ¤í‚µ
        if not force_rescan and (start_time - self.last_scan_time) < timedelta(hours=1):
            return {"status": "skipped", "reason": "Recent scan exists"}
        
        scan_results = {
            "start_time": start_time.isoformat(),
            "files_discovered": 0,
            "files_updated": 0,
            "files_deleted": 0,
            "relationships_found": 0,
            "errors": []
        }
        
        try:
            # í˜„ì¬ íŒŒì¼ë“¤ ìŠ¤ìº”
            current_files = set()
            
            for file_path in self._scan_directory(self.project_root):
                try:
                    current_files.add(str(file_path))
                    
                    # íŒŒì¼ì´ ì œì™¸ íŒ¨í„´ì— í•´ë‹¹í•˜ëŠ”ì§€ í™•ì¸
                    if self._should_exclude_file(file_path):
                        continue
                    
                    # ë©”íƒ€ë°ì´í„° ìƒì„±/ì—…ë°ì´íŠ¸
                    metadata = self._analyze_file(file_path)
                    if metadata:
                        if self._is_new_or_modified(metadata):
                            self._save_file_metadata(metadata)
                            scan_results["files_updated"] += 1
                        scan_results["files_discovered"] += 1
                    
                except Exception as e:
                    scan_results["errors"].append(f"Error processing {file_path}: {str(e)}")
            
            # ì‚­ì œëœ íŒŒì¼ í™•ì¸
            existing_files = self._get_all_tracked_files()
            deleted_files = set(existing_files) - current_files
            
            for deleted_file in deleted_files:
                self._mark_file_deleted(deleted_file)
                scan_results["files_deleted"] += 1
            
            # íŒŒì¼ ê°„ ê´€ê³„ ë¶„ì„
            relationships_found = self._analyze_file_relationships(current_files)
            scan_results["relationships_found"] = relationships_found
            
            # ìŠ¤ìº” ì™„ë£Œ ì‹œê°„ ì—…ë°ì´íŠ¸
            self.last_scan_time = start_time
            
            scan_results["end_time"] = datetime.now().isoformat()
            scan_results["duration_seconds"] = (datetime.now() - start_time).total_seconds()
            scan_results["status"] = "completed"
            
            print(f"ğŸ“ íŒŒì¼ ìŠ¤ìº” ì™„ë£Œ: {scan_results['files_discovered']}ê°œ ë°œê²¬, {scan_results['files_updated']}ê°œ ì—…ë°ì´íŠ¸")
            
        except Exception as e:
            scan_results["status"] = "failed"
            scan_results["error"] = str(e)
            print(f"âš ï¸ íŒŒì¼ ìŠ¤ìº” ì‹¤íŒ¨: {str(e)}")
        
        return scan_results
    
    def smart_search(self, query: SearchQuery) -> List[FileMetadata]:
        """ì§€ëŠ¥ì  íŒŒì¼ ê²€ìƒ‰"""
        
        # SQL ì¿¼ë¦¬ êµ¬ì„±
        sql = "SELECT * FROM file_metadata WHERE file_status != 'deleted'"
        params = []
        
        # í‚¤ì›Œë“œ ê²€ìƒ‰
        if query.keywords:
            keyword_conditions = []
            for keyword in query.keywords:
                keyword_conditions.append("(file_name LIKE ? OR purpose LIKE ? OR tags LIKE ?)")
                params.extend([f'%{keyword}%', f'%{keyword}%', f'%{keyword}%'])
            sql += f" AND ({' OR '.join(keyword_conditions)})"
        
        # íŒŒì¼ íƒ€ì… í•„í„°
        if query.file_types:
            type_placeholders = ','.join(['?' for _ in query.file_types])
            sql += f" AND file_type IN ({type_placeholders})"
            params.extend([ft.value for ft in query.file_types])
        
        # ì—­í•  í•„í„°
        if query.roles:
            role_placeholders = ','.join(['?' for _ in query.roles])
            sql += f" AND role_ownership IN ({role_placeholders})"
            params.extend(query.roles)
        
        # í”„ë¡œì íŠ¸ ë‹¨ê³„ í•„í„°
        if query.project_phases:
            phase_placeholders = ','.join(['?' for _ in query.project_phases])
            sql += f" AND project_phase IN ({phase_placeholders})"
            params.extend(query.project_phases)
        
        # ìˆ˜ì • ì‹œê°„ í•„í„°
        if query.modified_since:
            sql += " AND modified_at >= ?"
            params.append(query.modified_since.isoformat())
        
        # í¬ê¸° ë²”ìœ„ í•„í„°
        if query.size_range:
            sql += " AND size_bytes BETWEEN ? AND ?"
            params.extend(query.size_range)
        
        # í’ˆì§ˆ ì„ê³„ê°’ í•„í„°
        if query.quality_threshold:
            sql += " AND quality_score >= ?"
            params.append(query.quality_threshold)
        
        # ì ‘ê·¼ íŒ¨í„´ í•„í„°
        if query.access_pattern:
            sql += " AND access_pattern = ?"
            params.append(query.access_pattern.value)
        
        # íƒœê·¸ í•„í„°
        if query.tags:
            for tag in query.tags:
                sql += " AND tags LIKE ?"
                params.append(f'%"{tag}"%')
        
        # ì •ë ¬
        sql += " ORDER BY quality_score DESC, modified_at DESC"
        
        # ì œí•œ
        if query.limit:
            sql += " LIMIT ?"
            params.append(query.limit)
        
        # ì¿¼ë¦¬ ì‹¤í–‰
        with sqlite3.connect(self.metadata_db) as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.execute(sql, params)
            rows = cursor.fetchall()
        
        # FileMetadata ê°ì²´ë¡œ ë³€í™˜
        results = []
        for row in rows:
            metadata = self._row_to_file_metadata(row)
            
            # ì»¨í…ì¸  ì •ê·œì‹ í•„í„° (íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°)
            if query.content_regex and Path(metadata.file_path).exists():
                if not self._content_matches_regex(metadata.file_path, query.content_regex):
                    continue
            
            # ì œì™¸ íŒ¨í„´ í™•ì¸
            if query.exclude_patterns:
                if any(fnmatch.fnmatch(metadata.file_path, pattern) for pattern in query.exclude_patterns):
                    continue
            
            results.append(metadata)
        
        return results
    
    def suggest_relevant_files(self, 
                             role_id: str,
                             current_task: str,
                             max_suggestions: int = 10) -> List[Tuple[FileMetadata, float]]:
        """ê´€ë ¨ íŒŒì¼ ì œì•ˆ"""
        
        # í˜„ì¬ ì‘ì—…ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = self._extract_keywords_from_task(current_task)
        
        # ì—­í•  ê¸°ë°˜ ê²€ìƒ‰
        base_query = SearchQuery(
            keywords=keywords[:5],
            roles=[role_id],
            file_types=[FileType.SOURCE_CODE, FileType.DOCUMENTATION, FileType.SPECIFICATION],
            quality_threshold=0.3,
            limit=max_suggestions * 2
        )
        
        candidates = self.smart_search(base_query)
        
        # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
        scored_files = []
        for file_metadata in candidates:
            relevance_score = self._calculate_file_relevance(current_task, file_metadata)
            scored_files.append((file_metadata, relevance_score))
        
        # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        scored_files.sort(key=lambda x: x[1], reverse=True)
        
        return scored_files[:max_suggestions]
    
    def get_file_dependencies(self, file_path: str) -> List[FileRelationship]:
        """íŒŒì¼ ì˜ì¡´ì„± ì¡°íšŒ"""
        
        with sqlite3.connect(self.metadata_db) as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.execute('''
                SELECT * FROM file_relationships 
                WHERE from_file = ? OR to_file = ?
                ORDER BY strength DESC
            ''', (file_path, file_path))
            
            relationships = []
            for row in cursor.fetchall():
                relationships.append(FileRelationship(
                    from_file=row['from_file'],
                    to_file=row['to_file'],
                    relationship_type=row['relationship_type'],
                    strength=row['strength'],
                    discovered_at=datetime.fromisoformat(row['discovered_at'])
                ))
        
        return relationships
    
    def track_file_access(self, file_path: str, accessed_by: str, access_type: str = "read"):
        """íŒŒì¼ ì ‘ê·¼ ì¶”ì """
        
        # ì ‘ê·¼ ë¡œê·¸ ì €ì¥
        with sqlite3.connect(self.metadata_db) as conn:
            conn.execute('''
                INSERT INTO access_log (file_path, accessed_by, access_type, accessed_at)
                VALUES (?, ?, ?, ?)
            ''', (file_path, accessed_by, access_type, datetime.now().isoformat()))
            
            # íŒŒì¼ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            conn.execute('''
                UPDATE file_metadata 
                SET access_count = access_count + 1,
                    last_accessed = ?
                WHERE file_path = ?
            ''', (datetime.now().isoformat(), file_path))
        
        # ì ‘ê·¼ íŒ¨í„´ ì—…ë°ì´íŠ¸
        self._update_access_pattern(file_path)
    
    def get_project_file_overview(self) -> Dict[str, Any]:
        """í”„ë¡œì íŠ¸ íŒŒì¼ ê°œìš”"""
        
        with sqlite3.connect(self.metadata_db) as conn:
            # ì „ì²´ í†µê³„
            total_files = conn.execute("SELECT COUNT(*) FROM file_metadata WHERE file_status != 'deleted'").fetchone()[0]
            
            # íƒ€ì…ë³„ ë¶„í¬
            type_distribution = {}
            cursor = conn.execute('''
                SELECT file_type, COUNT(*) as count 
                FROM file_metadata 
                WHERE file_status != 'deleted'
                GROUP BY file_type
            ''')
            for row in cursor.fetchall():
                type_distribution[row[0]] = row[1]
            
            # ì—­í• ë³„ ë¶„í¬
            role_distribution = {}
            cursor = conn.execute('''
                SELECT role_ownership, COUNT(*) as count 
                FROM file_metadata 
                WHERE file_status != 'deleted' AND role_ownership IS NOT NULL
                GROUP BY role_ownership
            ''')
            for row in cursor.fetchall():
                role_distribution[row[0]] = row[1]
            
            # í’ˆì§ˆ ë¶„í¬
            quality_stats = conn.execute('''
                SELECT 
                    AVG(quality_score) as avg_quality,
                    MIN(quality_score) as min_quality,
                    MAX(quality_score) as max_quality
                FROM file_metadata 
                WHERE file_status != 'deleted'
            ''').fetchone()
            
            # ìµœê·¼ í™œë™
            recent_files = conn.execute('''
                SELECT file_path, modified_at 
                FROM file_metadata 
                WHERE file_status != 'deleted'
                ORDER BY modified_at DESC 
                LIMIT 10
            ''').fetchall()
            
            # ì ‘ê·¼ íŒ¨í„´
            access_patterns = {}
            cursor = conn.execute('''
                SELECT access_pattern, COUNT(*) as count 
                FROM file_metadata 
                WHERE file_status != 'deleted'
                GROUP BY access_pattern
            ''')
            for row in cursor.fetchall():
                access_patterns[row[0]] = row[1]
        
        return {
            'total_files': total_files,
            'type_distribution': type_distribution,
            'role_distribution': role_distribution,
            'quality_statistics': {
                'average_quality': quality_stats[0] or 0.0,
                'min_quality': quality_stats[1] or 0.0,
                'max_quality': quality_stats[2] or 0.0
            },
            'recent_activity': [
                {'file_path': row[0], 'modified_at': row[1]}
                for row in recent_files
            ],
            'access_patterns': access_patterns,
            'last_scan': self.last_scan_time.isoformat() if self.last_scan_time != datetime.min else None
        }
    
    # Helper methods
    def _scan_directory(self, directory: Path):
        """ë””ë ‰í† ë¦¬ ì¬ê·€ ìŠ¤ìº”"""
        for item in directory.rglob('*'):
            if item.is_file():
                yield item
    
    def _should_exclude_file(self, file_path: Path) -> bool:
        """íŒŒì¼ ì œì™¸ ì—¬ë¶€ í™•ì¸"""
        file_str = str(file_path)
        return any(fnmatch.fnmatch(file_str, pattern) for pattern in self.detection_rules['exclude_patterns'])
    
    def _analyze_file(self, file_path: Path) -> Optional[FileMetadata]:
        """íŒŒì¼ ë¶„ì„"""
        try:
            if not file_path.exists():
                return None
            
            stat = file_path.stat()
            
            # ê¸°ë³¸ ì •ë³´
            file_name = file_path.name
            size_bytes = stat.st_size
            created_at = datetime.fromtimestamp(stat.st_ctime)
            modified_at = datetime.fromtimestamp(stat.st_mtime)
            
            # íŒŒì¼ íƒ€ì… ê°ì§€
            file_type = self._detect_file_type(file_path)
            
            # ì½˜í…ì¸  í•´ì‹œ
            content_hash = self._calculate_file_hash(file_path)
            
            # MIME íƒ€ì…
            mime_type, _ = mimetypes.guess_type(str(file_path))
            mime_type = mime_type or 'application/octet-stream'
            
            # ì–¸ì–´ ê°ì§€
            language = self._detect_language(file_path)
            
            # í”„ë ˆì„ì›Œí¬ ê°ì§€
            framework = self._detect_framework(file_path)
            
            # ì˜ì¡´ì„± ì¶”ì¶œ
            dependencies = self._extract_dependencies(file_path)
            
            # íƒœê·¸ ìƒì„±
            tags = self._generate_file_tags(file_path, file_type)
            
            # ëª©ì  ì¶”ë¡ 
            purpose = self._infer_file_purpose(file_path, file_type)
            
            # ì—­í•  ì†Œìœ ê¶Œ ì¶”ë¡ 
            role_ownership = self._infer_role_ownership(file_path)
            
            # í”„ë¡œì íŠ¸ ë‹¨ê³„ ì¶”ë¡ 
            project_phase = self._infer_project_phase(file_path, file_type)
            
            # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            quality_score = self._calculate_quality_score(file_path, file_type)
            
            # ë³µì¡ë„ ì ìˆ˜ ê³„ì‚°
            complexity_score = self._calculate_complexity_score(file_path, file_type)
            
            # ìœ ì§€ë³´ìˆ˜ì„± ì ìˆ˜ ê³„ì‚°
            maintainability_score = self._calculate_maintainability_score(file_path, file_type)
            
            return FileMetadata(
                file_path=str(file_path),
                file_name=file_name,
                file_type=file_type,
                file_status=FileStatus.ACTIVE,
                size_bytes=size_bytes,
                created_at=created_at,
                modified_at=modified_at,
                last_accessed=None,
                access_count=0,
                access_pattern=AccessPattern.NEVER_ACCESSED,
                content_hash=content_hash,
                mime_type=mime_type,
                encoding=None,
                language=language,
                framework=framework,
                dependencies=dependencies,
                tags=tags,
                purpose=purpose,
                role_ownership=role_ownership,
                project_phase=project_phase,
                quality_score=quality_score,
                complexity_score=complexity_score,
                maintainability_score=maintainability_score
            )
            
        except Exception as e:
            print(f"íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜ ({file_path}): {str(e)}")
            return None
    
    def _detect_file_type(self, file_path: Path) -> FileType:
        """íŒŒì¼ íƒ€ì… ê°ì§€"""
        file_str = str(file_path).lower()
        
        for file_type, patterns in self.detection_rules['file_type_patterns'].items():
            if any(fnmatch.fnmatch(file_str, pattern.lower()) for pattern in patterns):
                return file_type
        
        return FileType.UNKNOWN
    
    def _calculate_file_hash(self, file_path: Path) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚°"""
        try:
            with open(file_path, 'rb') as f:
                content = f.read()
                return hashlib.md5(content).hexdigest()
        except:
            return ""
    
    def _detect_language(self, file_path: Path) -> Optional[str]:
        """í”„ë¡œê·¸ë˜ë° ì–¸ì–´ ê°ì§€"""
        suffix = file_path.suffix.lower()
        return self.detection_rules['language_detection'].get(suffix)
    
    def _detect_framework(self, file_path: Path) -> Optional[str]:
        """í”„ë ˆì„ì›Œí¬ ê°ì§€"""
        # ê°„ë‹¨í•œ êµ¬í˜„ - ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ë¶„ì„ í•„ìš”
        file_name = file_path.name.lower()
        
        for config_file, frameworks in self.detection_rules['framework_detection'].items():
            if config_file in file_name:
                return frameworks[0] if frameworks else None
        
        return None
    
    def _extract_dependencies(self, file_path: Path) -> List[str]:
        """ì˜ì¡´ì„± ì¶”ì¶œ"""
        dependencies = []
        
        try:
            if file_path.suffix.lower() == '.py':
                # Python import ë¶„ì„
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                    
                import_patterns = [
                    r'import\s+([a-zA-Z_][a-zA-Z0-9_]*)',
                    r'from\s+([a-zA-Z_][a-zA-Z0-9_]*)\s+import'
                ]
                
                for pattern in import_patterns:
                    matches = re.findall(pattern, content)
                    dependencies.extend(matches)
                    
        except Exception:
            pass
        
        return list(set(dependencies))
    
    def _generate_file_tags(self, file_path: Path, file_type: FileType) -> List[str]:
        """íŒŒì¼ íƒœê·¸ ìƒì„±"""
        tags = [file_type.value]
        
        file_name_lower = file_path.name.lower()
        
        # íŠ¹ë³„í•œ íŒŒì¼ë“¤ì— ëŒ€í•œ íƒœê·¸
        if 'test' in file_name_lower:
            tags.append('test')
        if 'config' in file_name_lower:
            tags.append('configuration')
        if 'readme' in file_name_lower:
            tags.append('documentation')
        if 'api' in file_name_lower:
            tags.append('api')
        if 'util' in file_name_lower or 'helper' in file_name_lower:
            tags.append('utility')
        
        # ê²½ë¡œ ê¸°ë°˜ íƒœê·¸
        path_parts = file_path.parts
        if 'src' in path_parts:
            tags.append('source')
        if 'test' in path_parts:
            tags.append('test')
        if 'doc' in path_parts or 'docs' in path_parts:
            tags.append('documentation')
        
        return list(set(tags))
    
    def _infer_file_purpose(self, file_path: Path, file_type: FileType) -> Optional[str]:
        """íŒŒì¼ ëª©ì  ì¶”ë¡ """
        file_name = file_path.name.lower()
        
        purpose_mapping = {
            'main': 'ë©”ì¸ ì§„ì…ì ',
            'index': 'ì¸ë±ìŠ¤ íŒŒì¼',
            'config': 'ì„¤ì • íŒŒì¼',
            'test': 'í…ŒìŠ¤íŠ¸ íŒŒì¼',
            'util': 'ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜',
            'helper': 'í—¬í¼ í•¨ìˆ˜',
            'model': 'ë°ì´í„° ëª¨ë¸',
            'view': 'ë·° ì»´í¬ë„ŒíŠ¸',
            'controller': 'ì»¨íŠ¸ë¡¤ëŸ¬',
            'service': 'ì„œë¹„ìŠ¤ ë¡œì§',
            'api': 'API ì¸í„°í˜ì´ìŠ¤',
            'component': 'UI ì»´í¬ë„ŒíŠ¸'
        }
        
        for keyword, purpose in purpose_mapping.items():
            if keyword in file_name:
                return purpose
        
        return None
    
    def _infer_role_ownership(self, file_path: Path) -> Optional[str]:
        """ì—­í•  ì†Œìœ ê¶Œ ì¶”ë¡ """
        path_str = str(file_path).lower()
        
        role_indicators = {
            'frontend': ['frontend', 'ui', 'component', 'view', 'css', 'html'],
            'backend': ['backend', 'api', 'server', 'service', 'model'],
            'qa_tester': ['test', 'spec', 'testing'],
            'devops': ['deploy', 'docker', 'kubernetes', 'ci', 'cd'],
            'database_designer': ['migration', 'schema', 'database', 'sql'],
            'business_analyst': ['requirement', 'spec', 'business']
        }
        
        for role, indicators in role_indicators.items():
            if any(indicator in path_str for indicator in indicators):
                return role
        
        return None
    
    def _infer_project_phase(self, file_path: Path, file_type: FileType) -> Optional[str]:
        """í”„ë¡œì íŠ¸ ë‹¨ê³„ ì¶”ë¡ """
        if file_type == FileType.SPECIFICATION:
            return 'planning'
        elif file_type == FileType.SOURCE_CODE:
            return 'development'
        elif file_type == FileType.TEST:
            return 'testing'
        elif file_type == FileType.CONFIGURATION and 'deploy' in str(file_path).lower():
            return 'deployment'
        elif file_type == FileType.DOCUMENTATION:
            return 'documentation'
        
        return None
    
    def _calculate_quality_score(self, file_path: Path, file_type: FileType) -> float:
        """í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        score = 0.5  # ê¸°ë³¸ ì ìˆ˜
        
        try:
            if file_type == FileType.SOURCE_CODE and file_path.exists():
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                    
                # ë¬¸ì„œí™” ì²´í¬
                if '"""' in content or '/*' in content or '//' in content:
                    score += 0.2
                
                # í…ŒìŠ¤íŠ¸ ê´€ë ¨ ì²´í¬
                if any(keyword in content.lower() for keyword in ['test', 'assert', 'expect']):
                    score += 0.1
                
                # ì—ëŸ¬ ì²˜ë¦¬ ì²´í¬
                if any(keyword in content.lower() for keyword in ['try', 'catch', 'except', 'error']):
                    score += 0.1
                
        except Exception:
            pass
        
        return min(1.0, score)
    
    def _calculate_complexity_score(self, file_path: Path, file_type: FileType) -> float:
        """ë³µì¡ë„ ì ìˆ˜ ê³„ì‚°"""
        if file_type != FileType.SOURCE_CODE or not file_path.exists():
            return 0.5
        
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
                
            # ë³µì¡ë„ ì§€í‘œ ì¹´ìš´íŠ¸
            complexity_keywords = self.detection_rules['quality_indicators']['complexity_keywords']
            complexity_count = sum(content.lower().count(keyword) for keyword in complexity_keywords)
            
            # ë¼ì¸ ìˆ˜ ê¸°ë°˜ ì •ê·œí™”
            line_count = len(content.split('\n'))
            if line_count > 0:
                complexity_ratio = complexity_count / line_count
                return min(1.0, complexity_ratio * 10)  # ìŠ¤ì¼€ì¼ë§
            
        except Exception:
            pass
        
        return 0.5
    
    def _calculate_maintainability_score(self, file_path: Path, file_type: FileType) -> float:
        """ìœ ì§€ë³´ìˆ˜ì„± ì ìˆ˜ ê³„ì‚°"""
        if file_type != FileType.SOURCE_CODE or not file_path.exists():
            return 0.5
        
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
                
            score = 0.5
            
            # êµ¬ì¡°í™” ì§€í‘œ
            maintainability_keywords = self.detection_rules['quality_indicators']['maintainability_keywords']
            structure_count = sum(content.lower().count(keyword) for keyword in maintainability_keywords)
            
            if structure_count > 0:
                score += 0.2
            
            # ì£¼ì„ ë¹„ìœ¨
            comment_lines = len([line for line in content.split('\n') if line.strip().startswith(('#', '//', '/*'))])
            total_lines = len(content.split('\n'))
            
            if total_lines > 0:
                comment_ratio = comment_lines / total_lines
                score += min(0.3, comment_ratio * 2)
            
            return min(1.0, score)
            
        except Exception:
            pass
        
        return 0.5
    
    def _monitor_file_changes(self):
        """íŒŒì¼ ë³€ê²½ ëª¨ë‹ˆí„°ë§"""
        while self.scan_active:
            try:
                # ê°„ë‹¨í•œ êµ¬í˜„ - ì‹¤ì œë¡œëŠ” íŒŒì¼ ì‹œìŠ¤í…œ ì´ë²¤íŠ¸ ëª¨ë‹ˆí„°ë§ ì‚¬ìš© ê¶Œì¥
                time.sleep(300)  # 5ë¶„ë§ˆë‹¤ ì²´í¬
                
                # ë³€ê²½ëœ íŒŒì¼ë§Œ ë‹¤ì‹œ ìŠ¤ìº”
                self._scan_modified_files()
                
            except Exception as e:
                print(f"íŒŒì¼ ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {str(e)}")
                time.sleep(60)
    
    def _scan_modified_files(self):
        """ìˆ˜ì •ëœ íŒŒì¼ë§Œ ìŠ¤ìº”"""
        # ìµœê·¼ ìŠ¤ìº” ì´í›„ ìˆ˜ì •ëœ íŒŒì¼ë“¤ë§Œ ë‹¤ì‹œ ë¶„ì„
        pass  # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” íŒŒì¼ ì‹œìŠ¤í…œ ë³€ê²½ ê°ì§€ êµ¬í˜„
    
    def _is_new_or_modified(self, metadata: FileMetadata) -> bool:
        """ìƒˆ íŒŒì¼ì´ê±°ë‚˜ ìˆ˜ì •ëœ íŒŒì¼ì¸ì§€ í™•ì¸"""
        with sqlite3.connect(self.metadata_db) as conn:
            cursor = conn.execute(
                "SELECT content_hash, modified_at FROM file_metadata WHERE file_path = ?",
                (metadata.file_path,)
            )
            row = cursor.fetchone()
            
            if not row:
                return True  # ìƒˆ íŒŒì¼
            
            # í•´ì‹œë‚˜ ìˆ˜ì • ì‹œê°„ì´ ë‹¤ë¥´ë©´ ìˆ˜ì •ëœ íŒŒì¼
            return (row[0] != metadata.content_hash or 
                   row[1] != metadata.modified_at.isoformat())
    
    def _save_file_metadata(self, metadata: FileMetadata):
        """íŒŒì¼ ë©”íƒ€ë°ì´í„° ì €ì¥"""
        with sqlite3.connect(self.metadata_db) as conn:
            conn.execute('''
                INSERT OR REPLACE INTO file_metadata 
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                metadata.file_path, metadata.file_name, metadata.file_type.value,
                metadata.file_status.value, metadata.size_bytes,
                metadata.created_at.isoformat(), metadata.modified_at.isoformat(),
                metadata.last_accessed.isoformat() if metadata.last_accessed else None,
                metadata.access_count, metadata.access_pattern.value,
                metadata.content_hash, metadata.mime_type, metadata.encoding,
                metadata.language, metadata.framework,
                json.dumps(metadata.dependencies), json.dumps(metadata.tags),
                metadata.purpose, metadata.role_ownership, metadata.project_phase,
                metadata.quality_score, metadata.complexity_score,
                metadata.maintainability_score,
                datetime.now().isoformat(), datetime.now().isoformat()
            ))
    
    def _get_all_tracked_files(self) -> List[str]:
        """ì¶”ì  ì¤‘ì¸ ëª¨ë“  íŒŒì¼ ëª©ë¡"""
        with sqlite3.connect(self.metadata_db) as conn:
            cursor = conn.execute("SELECT file_path FROM file_metadata WHERE file_status != 'deleted'")
            return [row[0] for row in cursor.fetchall()]
    
    def _mark_file_deleted(self, file_path: str):
        """íŒŒì¼ì„ ì‚­ì œë¨ìœ¼ë¡œ í‘œì‹œ"""
        with sqlite3.connect(self.metadata_db) as conn:
            conn.execute(
                "UPDATE file_metadata SET file_status = 'deleted', updated_at = ? WHERE file_path = ?",
                (datetime.now().isoformat(), file_path)
            )
    
    def _analyze_file_relationships(self, current_files: set) -> int:
        """íŒŒì¼ ê°„ ê´€ê³„ ë¶„ì„"""
        # ê°„ë‹¨í•œ êµ¬í˜„ - ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ì˜ì¡´ì„± ë¶„ì„ í•„ìš”
        relationships_found = 0
        
        # Python import ë¶„ì„ ì˜ˆì‹œ
        for file_path in current_files:
            if file_path.endswith('.py'):
                try:
                    relationships = self._analyze_python_imports(file_path)
                    for relationship in relationships:
                        self._save_file_relationship(relationship)
                        relationships_found += 1
                except Exception:
                    pass
        
        return relationships_found
    
    def _analyze_python_imports(self, file_path: str) -> List[FileRelationship]:
        """Python import ë¶„ì„"""
        relationships = []
        
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
                
            # ê°„ë‹¨í•œ import íŒ¨í„´ ë§¤ì¹­
            import_patterns = [
                r'from\s+\.([a-zA-Z_][a-zA-Z0-9_]*)\s+import',
                r'import\s+\.([a-zA-Z_][a-zA-Z0-9_]*)'
            ]
            
            for pattern in import_patterns:
                matches = re.findall(pattern, content)
                for match in matches:
                    # ìƒëŒ€ ê²½ë¡œë¡œ íŒŒì¼ ì°¾ê¸°
                    base_dir = Path(file_path).parent
                    imported_file = base_dir / f"{match}.py"
                    
                    if imported_file.exists():
                        relationship = FileRelationship(
                            from_file=file_path,
                            to_file=str(imported_file),
                            relationship_type="imports",
                            strength=0.8,
                            discovered_at=datetime.now()
                        )
                        relationships.append(relationship)
        
        except Exception:
            pass
        
        return relationships
    
    def _save_file_relationship(self, relationship: FileRelationship):
        """íŒŒì¼ ê´€ê³„ ì €ì¥"""
        with sqlite3.connect(self.metadata_db) as conn:
            conn.execute('''
                INSERT OR REPLACE INTO file_relationships 
                (from_file, to_file, relationship_type, strength, discovered_at)
                VALUES (?, ?, ?, ?, ?)
            ''', (
                relationship.from_file, relationship.to_file,
                relationship.relationship_type, relationship.strength,
                relationship.discovered_at.isoformat()
            ))
    
    def _row_to_file_metadata(self, row: sqlite3.Row) -> FileMetadata:
        """ë°ì´í„°ë² ì´ìŠ¤ í–‰ì„ FileMetadataë¡œ ë³€í™˜"""
        return FileMetadata(
            file_path=row['file_path'],
            file_name=row['file_name'],
            file_type=FileType(row['file_type']),
            file_status=FileStatus(row['file_status']),
            size_bytes=row['size_bytes'],
            created_at=datetime.fromisoformat(row['created_at']),
            modified_at=datetime.fromisoformat(row['modified_at']),
            last_accessed=datetime.fromisoformat(row['last_accessed']) if row['last_accessed'] else None,
            access_count=row['access_count'],
            access_pattern=AccessPattern(row['access_pattern']),
            content_hash=row['content_hash'],
            mime_type=row['mime_type'],
            encoding=row['encoding'],
            language=row['language'],
            framework=row['framework'],
            dependencies=json.loads(row['dependencies']) if row['dependencies'] else [],
            tags=json.loads(row['tags']) if row['tags'] else [],
            purpose=row['purpose'],
            role_ownership=row['role_ownership'],
            project_phase=row['project_phase'],
            quality_score=row['quality_score'],
            complexity_score=row['complexity_score'],
            maintainability_score=row['maintainability_score']
        )
    
    def _content_matches_regex(self, file_path: str, regex_pattern: str) -> bool:
        """íŒŒì¼ ì½˜í…ì¸ ê°€ ì •ê·œì‹ê³¼ ë§¤ì¹˜ë˜ëŠ”ì§€ í™•ì¸"""
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
                return bool(re.search(regex_pattern, content, re.IGNORECASE))
        except Exception:
            return False
    
    def _extract_keywords_from_task(self, task: str) -> List[str]:
        """ì‘ì—…ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
        words = re.findall(r'\b\w+\b', task.lower())
        
        # ë¶ˆìš©ì–´ ì œê±°
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
        keywords = [word for word in words if len(word) > 2 and word not in stop_words]
        
        return keywords[:10]
    
    def _calculate_file_relevance(self, task: str, file_metadata: FileMetadata) -> float:
        """íŒŒì¼ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        
        task_keywords = set(self._extract_keywords_from_task(task))
        
        # íŒŒì¼ëª… ë§¤ì¹­
        file_keywords = set(re.findall(r'\b\w+\b', file_metadata.file_name.lower()))
        name_overlap = len(task_keywords & file_keywords)
        if name_overlap > 0:
            score += 0.4 * (name_overlap / len(task_keywords))
        
        # íƒœê·¸ ë§¤ì¹­
        tag_overlap = len(task_keywords & set(file_metadata.tags))
        if tag_overlap > 0:
            score += 0.3 * (tag_overlap / len(task_keywords))
        
        # ëª©ì  ë§¤ì¹­
        if file_metadata.purpose:
            purpose_keywords = set(re.findall(r'\b\w+\b', file_metadata.purpose.lower()))
            purpose_overlap = len(task_keywords & purpose_keywords)
            if purpose_overlap > 0:
                score += 0.2 * (purpose_overlap / len(task_keywords))
        
        # í’ˆì§ˆ ì ìˆ˜ ê°€ì¤‘ì¹˜
        score *= file_metadata.quality_score
        
        return score
    
    def _update_access_pattern(self, file_path: str):
        """ì ‘ê·¼ íŒ¨í„´ ì—…ë°ì´íŠ¸"""
        with sqlite3.connect(self.metadata_db) as conn:
            # ìµœê·¼ 30ì¼ ì ‘ê·¼ íšŸìˆ˜ í™•ì¸
            thirty_days_ago = (datetime.now() - timedelta(days=30)).isoformat()
            
            cursor = conn.execute('''
                SELECT COUNT(*) FROM access_log 
                WHERE file_path = ? AND accessed_at >= ?
            ''', (file_path, thirty_days_ago))
            
            recent_access_count = cursor.fetchone()[0]
            
            # ì ‘ê·¼ íŒ¨í„´ ê²°ì •
            if recent_access_count >= 20:
                pattern = AccessPattern.FREQUENT
            elif recent_access_count >= 5:
                pattern = AccessPattern.OCCASIONAL
            elif recent_access_count >= 1:
                pattern = AccessPattern.RARE
            else:
                pattern = AccessPattern.NEVER_ACCESSED
            
            # ì—…ë°ì´íŠ¸
            conn.execute('''
                UPDATE file_metadata 
                SET access_pattern = ? 
                WHERE file_path = ?
            ''', (pattern.value, file_path))

def main():
    """í…ŒìŠ¤íŠ¸ ë° ë°ëª¨"""
    discovery_system = SmartFileDiscoverySystem("/home/jungh/workspace/multi_claude_code_sample")
    
    # ì˜ˆì‹œ 1: í”„ë¡œì íŠ¸ íŒŒì¼ ìŠ¤ìº”
    scan_results = discovery_system.scan_project_files(force_rescan=True)
    print(f"ğŸ“ ìŠ¤ìº” ê²°ê³¼: {scan_results['files_discovered']}ê°œ íŒŒì¼ ë°œê²¬")
    
    # ì˜ˆì‹œ 2: íŒŒì¼ ê²€ìƒ‰
    search_query = SearchQuery(
        keywords=['system', 'communication'],
        file_types=[FileType.SOURCE_CODE, FileType.DOCUMENTATION],
        quality_threshold=0.5,
        limit=5
    )
    
    search_results = discovery_system.smart_search(search_query)
    print(f"ğŸ” ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ íŒŒì¼")
    for result in search_results[:3]:
        print(f"  - {result.file_name} (í’ˆì§ˆ: {result.quality_score:.2f})")
    
    # ì˜ˆì‹œ 3: ê´€ë ¨ íŒŒì¼ ì œì•ˆ
    suggestions = discovery_system.suggest_relevant_files(
        role_id="business_analyst",
        current_task="ì‚¬ìš©ì ì¸ì¦ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ë¶„ì„",
        max_suggestions=5
    )
    
    print(f"ğŸ’¡ ì œì•ˆëœ ê´€ë ¨ íŒŒì¼ {len(suggestions)}ê°œ:")
    for file_metadata, relevance_score in suggestions:
        print(f"  - {file_metadata.file_name} (ê´€ë ¨ì„±: {relevance_score:.2f})")
    
    # ì˜ˆì‹œ 4: í”„ë¡œì íŠ¸ ê°œìš”
    overview = discovery_system.get_project_file_overview()
    print("ğŸ“Š í”„ë¡œì íŠ¸ íŒŒì¼ ê°œìš”:")
    print(f"  - ì´ íŒŒì¼ ìˆ˜: {overview['total_files']}")
    print(f"  - í‰ê·  í’ˆì§ˆ: {overview['quality_statistics']['average_quality']:.2f}")
    print(f"  - íƒ€ì… ë¶„í¬: {overview['type_distribution']}")

if __name__ == "__main__":
    main()