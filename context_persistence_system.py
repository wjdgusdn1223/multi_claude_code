#!/usr/bin/env python3
"""
Context Persistence System for Multi-Agent Claude Code
ì—­í• ë³„ íˆìŠ¤í† ë¦¬ ë° ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ ê°•í™” ì‹œìŠ¤í…œ
"""

import os
import json
import yaml
import sqlite3
import pickle
import hashlib
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any, Union
from enum import Enum
from dataclasses import dataclass, asdict
import threading
import logging

class ContextType(Enum):
    """ì»¨í…ìŠ¤íŠ¸ íƒ€ì…"""
    DECISION_HISTORY = "decision_history"        # ì˜ì‚¬ê²°ì • ì´ë ¥
    TASK_EXECUTION = "task_execution"            # ì‘ì—… ì‹¤í–‰ ê¸°ë¡
    COMMUNICATION = "communication"              # ì†Œí†µ ì´ë ¥
    LEARNING_OUTCOME = "learning_outcome"        # í•™ìŠµ ê²°ê³¼
    ERROR_PATTERN = "error_pattern"              # ì˜¤ë¥˜ íŒ¨í„´
    SUCCESS_PATTERN = "success_pattern"          # ì„±ê³µ íŒ¨í„´
    KNOWLEDGE_BASE = "knowledge_base"            # ì§€ì‹ ë² ì´ìŠ¤
    WORKFLOW_STATE = "workflow_state"            # ì›Œí¬í”Œë¡œìš° ìƒíƒœ

class ContextScope(Enum):
    """ì»¨í…ìŠ¤íŠ¸ ë²”ìœ„"""
    ROLE_SPECIFIC = "role_specific"              # ì—­í• ë³„
    PROJECT_WIDE = "project_wide"                # í”„ë¡œì íŠ¸ ì „ì²´
    CROSS_PROJECT = "cross_project"              # í”„ë¡œì íŠ¸ ê°„
    SYSTEM_GLOBAL = "system_global"              # ì‹œìŠ¤í…œ ì „ì—­

@dataclass
class ContextEntry:
    """ì»¨í…ìŠ¤íŠ¸ ì—”íŠ¸ë¦¬"""
    entry_id: str
    context_type: ContextType
    context_scope: ContextScope
    role_id: str
    project_id: Optional[str]
    timestamp: datetime
    content: Dict[str, Any]
    metadata: Dict[str, Any]
    tags: List[str]
    importance_score: float
    retention_period: Optional[timedelta]
    related_entries: List[str]

@dataclass
class ContextQuery:
    """ì»¨í…ìŠ¤íŠ¸ ì¿¼ë¦¬"""
    role_id: Optional[str] = None
    context_types: Optional[List[ContextType]] = None
    time_range: Optional[tuple] = None
    tags: Optional[List[str]] = None
    importance_threshold: Optional[float] = None
    project_id: Optional[str] = None
    content_keywords: Optional[List[str]] = None
    limit: Optional[int] = None

class ContextPersistenceSystem:
    """ì»¨í…ìŠ¤íŠ¸ ì§€ì†ì„± ì‹œìŠ¤í…œ"""
    
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.context_dir = self.project_root / "context_storage"
        self.db_path = self.context_dir / "context.db"
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.context_dir.mkdir(exist_ok=True)
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        self._init_database()
        
        # ë©”ëª¨ë¦¬ ìºì‹œ
        self.memory_cache: Dict[str, ContextEntry] = {}
        self.cache_lock = threading.Lock()
        
        # ë¡œê¹… ì„¤ì •
        self.logger = self._setup_logging()
        
        # ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ ê·œì¹™
        self.processing_rules = self._initialize_processing_rules()
        
        print("ğŸ§  Context Persistence System ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute('''
                CREATE TABLE IF NOT EXISTS context_entries (
                    entry_id TEXT PRIMARY KEY,
                    context_type TEXT NOT NULL,
                    context_scope TEXT NOT NULL,
                    role_id TEXT NOT NULL,
                    project_id TEXT,
                    timestamp TEXT NOT NULL,
                    content_json TEXT NOT NULL,
                    metadata_json TEXT NOT NULL,
                    tags_json TEXT NOT NULL,
                    importance_score REAL NOT NULL,
                    retention_period TEXT,
                    related_entries_json TEXT,
                    content_hash TEXT NOT NULL,
                    created_at TEXT NOT NULL
                )
            ''')
            
            conn.execute('''
                CREATE INDEX IF NOT EXISTS idx_role_type 
                ON context_entries (role_id, context_type)
            ''')
            
            conn.execute('''
                CREATE INDEX IF NOT EXISTS idx_timestamp 
                ON context_entries (timestamp)
            ''')
            
            conn.execute('''
                CREATE INDEX IF NOT EXISTS idx_importance 
                ON context_entries (importance_score)
            ''')
            
            conn.execute('''
                CREATE TABLE IF NOT EXISTS context_relationships (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    from_entry_id TEXT NOT NULL,
                    to_entry_id TEXT NOT NULL,
                    relationship_type TEXT NOT NULL,
                    strength REAL NOT NULL,
                    created_at TEXT NOT NULL,
                    FOREIGN KEY (from_entry_id) REFERENCES context_entries (entry_id),
                    FOREIGN KEY (to_entry_id) REFERENCES context_entries (entry_id)
                )
            ''')
    
    def _setup_logging(self) -> logging.Logger:
        """ë¡œê¹… ì„¤ì •"""
        logger = logging.getLogger('context_persistence')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.FileHandler(self.context_dir / 'context.log')
            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def _initialize_processing_rules(self) -> Dict[str, Any]:
        """ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ ê·œì¹™ ì´ˆê¸°í™”"""
        return {
            'importance_calculation': {
                ContextType.DECISION_HISTORY: 0.9,
                ContextType.ERROR_PATTERN: 0.8,
                ContextType.SUCCESS_PATTERN: 0.8,
                ContextType.LEARNING_OUTCOME: 0.7,
                ContextType.TASK_EXECUTION: 0.6,
                ContextType.COMMUNICATION: 0.5,
                ContextType.WORKFLOW_STATE: 0.4,
                ContextType.KNOWLEDGE_BASE: 0.3
            },
            'retention_periods': {
                ContextType.DECISION_HISTORY: timedelta(days=365),
                ContextType.ERROR_PATTERN: timedelta(days=180),
                ContextType.SUCCESS_PATTERN: timedelta(days=180),
                ContextType.LEARNING_OUTCOME: timedelta(days=90),
                ContextType.TASK_EXECUTION: timedelta(days=30),
                ContextType.COMMUNICATION: timedelta(days=30),
                ContextType.WORKFLOW_STATE: timedelta(days=7),
                ContextType.KNOWLEDGE_BASE: None  # ì˜êµ¬ ë³´ì¡´
            },
            'auto_tagging_keywords': {
                'critical_decision': ['critical', 'important', 'decision', 'choose', 'select'],
                'error_handling': ['error', 'exception', 'failure', 'bug', 'issue'],
                'performance': ['performance', 'speed', 'optimization', 'efficiency'],
                'quality': ['quality', 'review', 'validation', 'testing', 'standard']
            }
        }
    
    def store_context(self, 
                     role_id: str,
                     context_type: ContextType,
                     content: Dict[str, Any],
                     context_scope: ContextScope = ContextScope.ROLE_SPECIFIC,
                     project_id: Optional[str] = None,
                     custom_tags: List[str] = None,
                     importance_override: Optional[float] = None) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ì €ì¥"""
        
        # ì—”íŠ¸ë¦¬ ìƒì„±
        entry_id = self._generate_entry_id(role_id, context_type)
        timestamp = datetime.now()
        
        # ì¤‘ìš”ë„ ê³„ì‚°
        importance_score = importance_override or self._calculate_importance(context_type, content)
        
        # ìë™ íƒœê¹…
        auto_tags = self._auto_tag_content(content)
        tags = list(set((custom_tags or []) + auto_tags))
        
        # ë³´ì¡´ ê¸°ê°„ ì„¤ì •
        retention_period = self.processing_rules['retention_periods'].get(context_type)
        
        # ê´€ë ¨ ì—”íŠ¸ë¦¬ ì°¾ê¸°
        related_entries = self._find_related_entries(role_id, context_type, content)
        
        context_entry = ContextEntry(
            entry_id=entry_id,
            context_type=context_type,
            context_scope=context_scope,
            role_id=role_id,
            project_id=project_id,
            timestamp=timestamp,
            content=content,
            metadata={
                'source': 'system_generated',
                'processing_version': '1.0',
                'content_size': len(str(content))
            },
            tags=tags,
            importance_score=importance_score,
            retention_period=retention_period,
            related_entries=related_entries
        )
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        self._save_to_database(context_entry)
        
        # ë©”ëª¨ë¦¬ ìºì‹œ ì—…ë°ì´íŠ¸
        with self.cache_lock:
            self.memory_cache[entry_id] = context_entry
        
        # ê´€ê³„ ì €ì¥
        self._store_relationships(context_entry)
        
        self.logger.info(f"Context stored: {entry_id} for role {role_id}")
        return entry_id
    
    def retrieve_context(self, query: ContextQuery) -> List[ContextEntry]:
        """ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ"""
        
        # SQL ì¿¼ë¦¬ êµ¬ì„±
        sql = "SELECT * FROM context_entries WHERE 1=1"
        params = []
        
        if query.role_id:
            sql += " AND role_id = ?"
            params.append(query.role_id)
        
        if query.context_types:
            type_placeholders = ','.join(['?' for _ in query.context_types])
            sql += f" AND context_type IN ({type_placeholders})"
            params.extend([ct.value for ct in query.context_types])
        
        if query.time_range:
            sql += " AND timestamp BETWEEN ? AND ?"
            params.extend([query.time_range[0].isoformat(), query.time_range[1].isoformat()])
        
        if query.importance_threshold:
            sql += " AND importance_score >= ?"
            params.append(query.importance_threshold)
        
        if query.project_id:
            sql += " AND project_id = ?"
            params.append(query.project_id)
        
        # íƒœê·¸ í•„í„°ë§
        if query.tags:
            for tag in query.tags:
                sql += " AND tags_json LIKE ?"
                params.append(f'%"{tag}"%')
        
        # í‚¤ì›Œë“œ í•„í„°ë§
        if query.content_keywords:
            for keyword in query.content_keywords:
                sql += " AND content_json LIKE ?"
                params.append(f'%{keyword}%')
        
        # ì •ë ¬ ë° ì œí•œ
        sql += " ORDER BY importance_score DESC, timestamp DESC"
        
        if query.limit:
            sql += " LIMIT ?"
            params.append(query.limit)
        
        # ì¿¼ë¦¬ ì‹¤í–‰
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.execute(sql, params)
            rows = cursor.fetchall()
        
        # ContextEntry ê°ì²´ë¡œ ë³€í™˜
        entries = []
        for row in rows:
            entry = self._row_to_context_entry(row)
            entries.append(entry)
        
        self.logger.info(f"Retrieved {len(entries)} context entries")
        return entries
    
    def get_role_context_summary(self, role_id: str, days_back: int = 30) -> Dict[str, Any]:
        """ì—­í• ë³„ ì»¨í…ìŠ¤íŠ¸ ìš”ì•½"""
        
        # ìµœê·¼ ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ
        time_range = (datetime.now() - timedelta(days=days_back), datetime.now())
        query = ContextQuery(
            role_id=role_id,
            time_range=time_range
        )
        
        entries = self.retrieve_context(query)
        
        # ìš”ì•½ ìƒì„±
        summary = {
            'role_id': role_id,
            'period': f"ìµœê·¼ {days_back}ì¼",
            'total_entries': len(entries),
            'by_type': {},
            'key_decisions': [],
            'learned_patterns': [],
            'frequent_errors': [],
            'success_factors': [],
            'communication_patterns': {},
            'performance_trends': [],
            'recommendations': []
        }
        
        # íƒ€ì…ë³„ ë¶„ë¥˜
        for entry in entries:
            entry_type = entry.context_type.value
            if entry_type not in summary['by_type']:
                summary['by_type'][entry_type] = 0
            summary['by_type'][entry_type] += 1
        
        # ì¤‘ìš”í•œ ì˜ì‚¬ê²°ì • ì¶”ì¶œ
        decision_entries = [e for e in entries if e.context_type == ContextType.DECISION_HISTORY]
        decision_entries.sort(key=lambda x: x.importance_score, reverse=True)
        for entry in decision_entries[:5]:
            summary['key_decisions'].append({
                'timestamp': entry.timestamp.isoformat(),
                'decision': entry.content.get('decision_summary', 'Unknown'),
                'importance': entry.importance_score
            })
        
        # í•™ìŠµ íŒ¨í„´ ì¶”ì¶œ
        learning_entries = [e for e in entries if e.context_type == ContextType.LEARNING_OUTCOME]
        for entry in learning_entries[-3:]:  # ìµœê·¼ 3ê°œ
            summary['learned_patterns'].append({
                'timestamp': entry.timestamp.isoformat(),
                'pattern': entry.content.get('pattern_description', 'Unknown'),
                'confidence': entry.content.get('confidence_score', 0.0)
            })
        
        # ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„
        error_entries = [e for e in entries if e.context_type == ContextType.ERROR_PATTERN]
        error_counts = {}
        for entry in error_entries:
            error_type = entry.content.get('error_type', 'unknown')
            error_counts[error_type] = error_counts.get(error_type, 0) + 1
        
        summary['frequent_errors'] = [
            {'error_type': k, 'count': v} 
            for k, v in sorted(error_counts.items(), key=lambda x: x[1], reverse=True)[:3]
        ]
        
        # ì„±ê³µ ìš”ì¸ ë¶„ì„
        success_entries = [e for e in entries if e.context_type == ContextType.SUCCESS_PATTERN]
        for entry in success_entries[-3:]:
            summary['success_factors'].append({
                'timestamp': entry.timestamp.isoformat(),
                'factor': entry.content.get('success_factor', 'Unknown'),
                'impact': entry.content.get('impact_score', 0.0)
            })
        
        # ê¶Œê³ ì‚¬í•­ ìƒì„±
        summary['recommendations'] = self._generate_recommendations(role_id, entries)
        
        return summary
    
    def store_decision_history(self, 
                             role_id: str,
                             decision_context: Dict[str, Any],
                             decision_made: str,
                             alternatives_considered: List[str],
                             decision_rationale: str,
                             confidence_level: float,
                             project_id: Optional[str] = None) -> str:
        """ì˜ì‚¬ê²°ì • ì´ë ¥ ì €ì¥"""
        
        content = {
            'decision_context': decision_context,
            'decision_made': decision_made,
            'alternatives_considered': alternatives_considered,
            'decision_rationale': decision_rationale,
            'confidence_level': confidence_level,
            'decision_summary': f"{decision_made} (ì‹ ë¢°ë„: {confidence_level:.2f})"
        }
        
        return self.store_context(
            role_id=role_id,
            context_type=ContextType.DECISION_HISTORY,
            content=content,
            project_id=project_id,
            custom_tags=['decision', 'critical'] if confidence_level > 0.8 else ['decision'],
            importance_override=0.9 if confidence_level > 0.8 else None
        )
    
    def store_task_execution(self,
                           role_id: str,
                           task_description: str,
                           execution_steps: List[str],
                           execution_result: Dict[str, Any],
                           performance_metrics: Dict[str, Any],
                           project_id: Optional[str] = None) -> str:
        """ì‘ì—… ì‹¤í–‰ ê¸°ë¡ ì €ì¥"""
        
        content = {
            'task_description': task_description,
            'execution_steps': execution_steps,
            'execution_result': execution_result,
            'performance_metrics': performance_metrics,
            'success': execution_result.get('success', False),
            'execution_time': performance_metrics.get('execution_time', 0),
            'quality_score': execution_result.get('quality_score', 0.0)
        }
        
        tags = ['task_execution']
        if content['success']:
            tags.append('successful')
        else:
            tags.append('failed')
        
        return self.store_context(
            role_id=role_id,
            context_type=ContextType.TASK_EXECUTION,
            content=content,
            project_id=project_id,
            custom_tags=tags
        )
    
    def store_learning_outcome(self,
                             role_id: str,
                             learning_context: str,
                             pattern_discovered: str,
                             pattern_details: Dict[str, Any],
                             confidence_score: float,
                             applicable_scenarios: List[str],
                             project_id: Optional[str] = None) -> str:
        """í•™ìŠµ ê²°ê³¼ ì €ì¥"""
        
        content = {
            'learning_context': learning_context,
            'pattern_discovered': pattern_discovered,
            'pattern_details': pattern_details,
            'confidence_score': confidence_score,
            'applicable_scenarios': applicable_scenarios,
            'pattern_description': f"{pattern_discovered} (ì‹ ë¢°ë„: {confidence_score:.2f})"
        }
        
        return self.store_context(
            role_id=role_id,
            context_type=ContextType.LEARNING_OUTCOME,
            content=content,
            project_id=project_id,
            custom_tags=['learning', 'pattern', 'intelligence']
        )
    
    def store_error_pattern(self,
                          role_id: str,
                          error_type: str,
                          error_details: Dict[str, Any],
                          occurrence_frequency: int,
                          resolution_steps: List[str],
                          prevention_strategies: List[str],
                          project_id: Optional[str] = None) -> str:
        """ì˜¤ë¥˜ íŒ¨í„´ ì €ì¥"""
        
        content = {
            'error_type': error_type,
            'error_details': error_details,
            'occurrence_frequency': occurrence_frequency,
            'resolution_steps': resolution_steps,
            'prevention_strategies': prevention_strategies,
            'severity': error_details.get('severity', 'medium')
        }
        
        tags = ['error', 'pattern']
        if content['severity'] == 'high':
            tags.append('critical')
        
        return self.store_context(
            role_id=role_id,
            context_type=ContextType.ERROR_PATTERN,
            content=content,
            project_id=project_id,
            custom_tags=tags,
            importance_override=0.9 if content['severity'] == 'high' else None
        )
    
    def get_relevant_context_for_task(self, 
                                    role_id: str,
                                    current_task_description: str,
                                    max_entries: int = 10) -> List[ContextEntry]:
        """í˜„ì¬ ì‘ì—…ì— ê´€ë ¨ëœ ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ"""
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = self._extract_keywords(current_task_description)
        
        # ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ì¿¼ë¦¬
        query = ContextQuery(
            role_id=role_id,
            content_keywords=keywords[:5],  # ìƒìœ„ 5ê°œ í‚¤ì›Œë“œ
            importance_threshold=0.3,
            limit=max_entries * 2  # í•„í„°ë§ì„ ìœ„í•´ ë” ë§ì´ ì¡°íšŒ
        )
        
        entries = self.retrieve_context(query)
        
        # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° ë° ì •ë ¬
        scored_entries = []
        for entry in entries:
            relevance_score = self._calculate_relevance_score(
                current_task_description, entry
            )
            scored_entries.append((entry, relevance_score))
        
        # ê´€ë ¨ì„± ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        scored_entries.sort(key=lambda x: x[1], reverse=True)
        
        # ìƒìœ„ í•­ëª© ë°˜í™˜
        return [entry for entry, score in scored_entries[:max_entries]]
    
    # Helper methods
    def _generate_entry_id(self, role_id: str, context_type: ContextType) -> str:
        """ì—”íŠ¸ë¦¬ ID ìƒì„±"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')
        return f"{role_id}_{context_type.value}_{timestamp}"
    
    def _calculate_importance(self, context_type: ContextType, content: Dict[str, Any]) -> float:
        """ì¤‘ìš”ë„ ê³„ì‚°"""
        base_importance = self.processing_rules['importance_calculation'].get(context_type, 0.5)
        
        # ì»¨í…ì¸  ê¸°ë°˜ ì¡°ì •
        adjustments = 0.0
        
        # ì„±ê³µ/ì‹¤íŒ¨ì— ë”°ë¥¸ ì¡°ì •
        if content.get('success') is False:
            adjustments += 0.1  # ì‹¤íŒ¨ëŠ” í•™ìŠµ ê¸°íšŒ
        elif content.get('success') is True:
            adjustments += 0.05  # ì„±ê³µë„ ì¤‘ìš”
        
        # ì‹ ë¢°ë„/í’ˆì§ˆ ì ìˆ˜ì— ë”°ë¥¸ ì¡°ì •
        confidence = content.get('confidence_level') or content.get('confidence_score', 0.5)
        if confidence > 0.8:
            adjustments += 0.1
        
        quality_score = content.get('quality_score', 0.5)
        if quality_score > 0.8:
            adjustments += 0.05
        
        return min(1.0, base_importance + adjustments)
    
    def _auto_tag_content(self, content: Dict[str, Any]) -> List[str]:
        """ì»¨í…ì¸  ìë™ íƒœê¹…"""
        tags = []
        content_text = str(content).lower()
        
        for tag, keywords in self.processing_rules['auto_tagging_keywords'].items():
            if any(keyword in content_text for keyword in keywords):
                tags.append(tag)
        
        return tags
    
    def _find_related_entries(self, 
                            role_id: str, 
                            context_type: ContextType, 
                            content: Dict[str, Any]) -> List[str]:
        """ê´€ë ¨ ì—”íŠ¸ë¦¬ ì°¾ê¸°"""
        # ê°„ë‹¨í•œ êµ¬í˜„ - ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ìœ ì‚¬ì„± ë¶„ì„ í•„ìš”
        keywords = self._extract_keywords(str(content))[:3]
        
        if not keywords:
            return []
        
        query = ContextQuery(
            role_id=role_id,
            content_keywords=keywords,
            limit=5
        )
        
        related_entries = self.retrieve_context(query)
        return [entry.entry_id for entry in related_entries[-5:]]  # ìµœê·¼ 5ê°œ
    
    def _extract_keywords(self, text: str) -> List[str]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ - ì‹¤ì œë¡œëŠ” NLP ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ê¶Œì¥
        words = text.lower().split()
        
        # ë¶ˆìš©ì–´ ì œê±°
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
        keywords = [word for word in words if len(word) > 3 and word not in stop_words]
        
        # ë¹ˆë„ ê¸°ë°˜ ì •ë ¬ (ê°„ë‹¨í•œ êµ¬í˜„)
        from collections import Counter
        word_counts = Counter(keywords)
        
        return [word for word, count in word_counts.most_common(10)]
    
    def _calculate_relevance_score(self, task_description: str, entry: ContextEntry) -> float:
        """ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        task_keywords = set(self._extract_keywords(task_description))
        entry_keywords = set(self._extract_keywords(str(entry.content)))
        
        # í‚¤ì›Œë“œ êµì§‘í•© ê¸°ë°˜ ê´€ë ¨ì„±
        if not task_keywords or not entry_keywords:
            return 0.0
        
        intersection = len(task_keywords & entry_keywords)
        union = len(task_keywords | entry_keywords)
        
        keyword_similarity = intersection / union if union > 0 else 0.0
        
        # ì¤‘ìš”ë„ì™€ ì‹œê°„ ê°€ì¤‘ì¹˜ ì ìš©
        time_weight = max(0.1, 1.0 - (datetime.now() - entry.timestamp).days / 365)
        
        return keyword_similarity * entry.importance_score * time_weight
    
    def _save_to_database(self, entry: ContextEntry):
        """ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        content_hash = hashlib.md5(str(entry.content).encode()).hexdigest()
        
        with sqlite3.connect(self.db_path) as conn:
            conn.execute('''
                INSERT OR REPLACE INTO context_entries 
                (entry_id, context_type, context_scope, role_id, project_id, timestamp,
                 content_json, metadata_json, tags_json, importance_score, 
                 retention_period, related_entries_json, content_hash, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                entry.entry_id,
                entry.context_type.value,
                entry.context_scope.value,
                entry.role_id,
                entry.project_id,
                entry.timestamp.isoformat(),
                json.dumps(entry.content, ensure_ascii=False),
                json.dumps(entry.metadata, ensure_ascii=False),
                json.dumps(entry.tags, ensure_ascii=False),
                entry.importance_score,
                entry.retention_period.total_seconds() if entry.retention_period else None,
                json.dumps(entry.related_entries, ensure_ascii=False),
                content_hash,
                datetime.now().isoformat()
            ))
    
    def _store_relationships(self, entry: ContextEntry):
        """ê´€ê³„ ì €ì¥"""
        if not entry.related_entries:
            return
        
        with sqlite3.connect(self.db_path) as conn:
            for related_id in entry.related_entries:
                conn.execute('''
                    INSERT INTO context_relationships 
                    (from_entry_id, to_entry_id, relationship_type, strength, created_at)
                    VALUES (?, ?, ?, ?, ?)
                ''', (
                    entry.entry_id,
                    related_id,
                    'content_similarity',
                    0.7,  # ê¸°ë³¸ ê°•ë„
                    datetime.now().isoformat()
                ))
    
    def _row_to_context_entry(self, row: sqlite3.Row) -> ContextEntry:
        """ë°ì´í„°ë² ì´ìŠ¤ í–‰ì„ ContextEntryë¡œ ë³€í™˜"""
        return ContextEntry(
            entry_id=row['entry_id'],
            context_type=ContextType(row['context_type']),
            context_scope=ContextScope(row['context_scope']),
            role_id=row['role_id'],
            project_id=row['project_id'],
            timestamp=datetime.fromisoformat(row['timestamp']),
            content=json.loads(row['content_json']),
            metadata=json.loads(row['metadata_json']),
            tags=json.loads(row['tags_json']),
            importance_score=row['importance_score'],
            retention_period=timedelta(seconds=row['retention_period']) if row['retention_period'] else None,
            related_entries=json.loads(row['related_entries_json'])
        )
    
    def _generate_recommendations(self, role_id: str, entries: List[ContextEntry]) -> List[str]:
        """ê¶Œê³ ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ì˜¤ë¥˜ íŒ¨í„´ ê¸°ë°˜ ê¶Œê³ 
        error_entries = [e for e in entries if e.context_type == ContextType.ERROR_PATTERN]
        if len(error_entries) > 3:
            recommendations.append("ë°˜ë³µë˜ëŠ” ì˜¤ë¥˜ íŒ¨í„´ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ì˜ˆë°© ì „ëµì„ ê°•í™”í•˜ì„¸ìš”.")
        
        # ì„±ê³µ íŒ¨í„´ ê¸°ë°˜ ê¶Œê³ 
        success_entries = [e for e in entries if e.context_type == ContextType.SUCCESS_PATTERN]
        if success_entries:
            recommendations.append("ì„±ê³µì ì¸ íŒ¨í„´ì„ ë‹¤ë¥¸ ì‘ì—…ì—ë„ ì ìš©í•´ë³´ì„¸ìš”.")
        
        # ì˜ì‚¬ê²°ì • íŒ¨í„´ ë¶„ì„
        decision_entries = [e for e in entries if e.context_type == ContextType.DECISION_HISTORY]
        low_confidence_decisions = [e for e in decision_entries if e.content.get('confidence_level', 1.0) < 0.6]
        if len(low_confidence_decisions) > 2:
            recommendations.append("ì‹ ë¢°ë„ê°€ ë‚®ì€ ì˜ì‚¬ê²°ì •ì´ ë§ìŠµë‹ˆë‹¤. ë” ë§ì€ ì •ë³´ ìˆ˜ì§‘ì„ ê³ ë ¤í•˜ì„¸ìš”.")
        
        return recommendations

def main():
    """í…ŒìŠ¤íŠ¸ ë° ë°ëª¨"""
    context_system = ContextPersistenceSystem("/home/jungh/workspace/multi_claude_code_sample")
    
    # ì˜ˆì‹œ 1: ì˜ì‚¬ê²°ì • ì´ë ¥ ì €ì¥
    decision_id = context_system.store_decision_history(
        role_id="business_analyst",
        decision_context={
            "situation": "API ë²„ì „ ì„ íƒ",
            "constraints": ["ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­", "í˜¸í™˜ì„±", "ë³´ì•ˆ"],
            "stakeholders": ["ê°œë°œíŒ€", "ìš´ì˜íŒ€"]
        },
        decision_made="REST API v2.0 ì‚¬ìš©",
        alternatives_considered=["GraphQL", "REST API v1.0", "gRPC"],
        decision_rationale="ì„±ëŠ¥ê³¼ í˜¸í™˜ì„±ì˜ ê· í˜•ì´ ê°€ì¥ ì¢‹ìŒ",
        confidence_level=0.85,
        project_id="web_community"
    )
    print(f"âœ… ì˜ì‚¬ê²°ì • ì´ë ¥ ì €ì¥: {decision_id}")
    
    # ì˜ˆì‹œ 2: ì‘ì—… ì‹¤í–‰ ê¸°ë¡ ì €ì¥
    task_id = context_system.store_task_execution(
        role_id="frontend_developer",
        task_description="ì‚¬ìš©ì ë¡œê·¸ì¸ í¼ êµ¬í˜„",
        execution_steps=[
            "ìš”êµ¬ì‚¬í•­ ë¶„ì„",
            "UI ì»´í¬ë„ŒíŠ¸ ì„¤ê³„",
            "í¼ ê²€ì¦ ë¡œì§ êµ¬í˜„",
            "API í†µí•©",
            "í…ŒìŠ¤íŠ¸ ìˆ˜í–‰"
        ],
        execution_result={
            "success": True,
            "deliverable_path": "/src/components/LoginForm.tsx",
            "quality_score": 0.92
        },
        performance_metrics={
            "execution_time": 240,  # ë¶„
            "code_coverage": 0.85,
            "bugs_found": 1
        },
        project_id="web_community"
    )
    print(f"âœ… ì‘ì—… ì‹¤í–‰ ê¸°ë¡ ì €ì¥: {task_id}")
    
    # ì˜ˆì‹œ 3: í•™ìŠµ ê²°ê³¼ ì €ì¥
    learning_id = context_system.store_learning_outcome(
        role_id="qa_tester",
        learning_context="ìë™í™” í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ë°œê²¬ëœ íŒ¨í„´",
        pattern_discovered="API ì‘ë‹µ ì‹œê°„ì´ DB ì¿¼ë¦¬ ë³µì¡ë„ì™€ ê°•í•œ ìƒê´€ê´€ê³„",
        pattern_details={
            "correlation_coefficient": 0.87,
            "sample_size": 150,
            "statistical_significance": 0.001
        },
        confidence_score=0.9,
        applicable_scenarios=["ì„±ëŠ¥ í…ŒìŠ¤íŠ¸", "API ìµœì í™”", "ë°ì´í„°ë² ì´ìŠ¤ íŠœë‹"],
        project_id="web_community"
    )
    print(f"âœ… í•™ìŠµ ê²°ê³¼ ì €ì¥: {learning_id}")
    
    # ì˜ˆì‹œ 4: ì—­í• ë³„ ì»¨í…ìŠ¤íŠ¸ ìš”ì•½ ì¡°íšŒ
    summary = context_system.get_role_context_summary("business_analyst", days_back=7)
    print("ğŸ“Š Business Analyst ì»¨í…ìŠ¤íŠ¸ ìš”ì•½:")
    print(f"  - ì´ ì—”íŠ¸ë¦¬: {summary['total_entries']}")
    print(f"  - ì£¼ìš” ì˜ì‚¬ê²°ì •: {len(summary['key_decisions'])}ê°œ")
    print(f"  - ê¶Œê³ ì‚¬í•­: {len(summary['recommendations'])}ê°œ")
    
    # ì˜ˆì‹œ 5: í˜„ì¬ ì‘ì—… ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ
    relevant_context = context_system.get_relevant_context_for_task(
        role_id="frontend_developer",
        current_task_description="ì‚¬ìš©ì ëŒ€ì‹œë³´ë“œ ì»´í¬ë„ŒíŠ¸ êµ¬í˜„",
        max_entries=5
    )
    print(f"ğŸ” ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ {len(relevant_context)}ê°œ ë°œê²¬")

if __name__ == "__main__":
    main()