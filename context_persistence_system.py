#!/usr/bin/env python3
"""
Context Persistence System for Multi-Agent Claude Code
Ïó≠Ìï†Î≥Ñ ÌûàÏä§ÌÜ†Î¶¨ Î∞è Ïª®ÌÖçÏä§Ìä∏ Í¥ÄÎ¶¨ Í∞ïÌôî ÏãúÏä§ÌÖú
"""

import os
import json
import yaml
import sqlite3
import pickle
import hashlib
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any, Union
from enum import Enum
from dataclasses import dataclass, asdict
import threading
import logging

class ContextType(Enum):
    """Ïª®ÌÖçÏä§Ìä∏ ÌÉÄÏûÖ"""
    DECISION_HISTORY = "decision_history"        # ÏùòÏÇ¨Í≤∞Ï†ï Ïù¥Î†•
    TASK_EXECUTION = "task_execution"            # ÏûëÏóÖ Ïã§Ìñâ Í∏∞Î°ù
    COMMUNICATION = "communication"              # ÏÜåÌÜµ Ïù¥Î†•
    LEARNING_OUTCOME = "learning_outcome"        # ÌïôÏäµ Í≤∞Í≥º
    ERROR_PATTERN = "error_pattern"              # Ïò§Î•ò Ìå®ÌÑ¥
    SUCCESS_PATTERN = "success_pattern"          # ÏÑ±Í≥µ Ìå®ÌÑ¥
    KNOWLEDGE_BASE = "knowledge_base"            # ÏßÄÏãù Î≤†Ïù¥Ïä§
    WORKFLOW_STATE = "workflow_state"            # ÏõåÌÅ¨ÌîåÎ°úÏö∞ ÏÉÅÌÉú

class ContextScope(Enum):
    """Ïª®ÌÖçÏä§Ìä∏ Î≤îÏúÑ"""
    ROLE_SPECIFIC = "role_specific"              # Ïó≠Ìï†Î≥Ñ
    PROJECT_WIDE = "project_wide"                # ÌîÑÎ°úÏ†ùÌä∏ Ï†ÑÏ≤¥
    CROSS_PROJECT = "cross_project"              # ÌîÑÎ°úÏ†ùÌä∏ Í∞Ñ
    SYSTEM_GLOBAL = "system_global"              # ÏãúÏä§ÌÖú Ï†ÑÏó≠

@dataclass
class ContextEntry:
    """Ïª®ÌÖçÏä§Ìä∏ ÏóîÌä∏Î¶¨"""
    entry_id: str
    context_type: ContextType
    context_scope: ContextScope
    role_id: str
    project_id: Optional[str]
    timestamp: datetime
    content: Dict[str, Any]
    metadata: Dict[str, Any]
    tags: List[str]
    importance_score: float
    retention_period: Optional[timedelta]
    related_entries: List[str]

@dataclass
class ContextQuery:
    """Ïª®ÌÖçÏä§Ìä∏ ÏøºÎ¶¨"""
    role_id: Optional[str] = None
    context_types: Optional[List[ContextType]] = None
    time_range: Optional[tuple] = None
    tags: Optional[List[str]] = None
    importance_threshold: Optional[float] = None
    project_id: Optional[str] = None
    content_keywords: Optional[List[str]] = None
    limit: Optional[int] = None

class ContextPersistenceSystem:
    """Ïª®ÌÖçÏä§Ìä∏ ÏßÄÏÜçÏÑ± ÏãúÏä§ÌÖú"""
    
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.context_dir = self.project_root / "context_storage"
        self.db_path = self.context_dir / "context.db"
        
        # ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±
        self.context_dir.mkdir(exist_ok=True)
        
        # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ï¥àÍ∏∞Ìôî
        self._init_database()
        
        # Î©îÎ™®Î¶¨ Ï∫êÏãú
        self.memory_cache: Dict[str, ContextEntry] = {}
        self.cache_lock = threading.Lock()
        
        # Î°úÍπÖ ÏÑ§Ï†ï
        self.logger = self._setup_logging()
        
        # Ïª®ÌÖçÏä§Ìä∏ Ï≤òÎ¶¨ Í∑úÏπô
        self.processing_rules = self._initialize_processing_rules()
        
        print("üß† Context Persistence System Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
    
    def _init_database(self):
        """Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ï¥àÍ∏∞Ìôî"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute('''
                CREATE TABLE IF NOT EXISTS context_entries (
                    entry_id TEXT PRIMARY KEY,
                    context_type TEXT NOT NULL,
                    context_scope TEXT NOT NULL,
                    role_id TEXT NOT NULL,
                    project_id TEXT,
                    timestamp TEXT NOT NULL,
                    content_json TEXT NOT NULL,
                    metadata_json TEXT NOT NULL,
                    tags_json TEXT NOT NULL,
                    importance_score REAL NOT NULL,
                    retention_period TEXT,
                    related_entries_json TEXT,
                    content_hash TEXT NOT NULL,
                    created_at TEXT NOT NULL
                )
            ''')
            
            conn.execute('''
                CREATE INDEX IF NOT EXISTS idx_role_type 
                ON context_entries (role_id, context_type)
            ''')
            
            conn.execute('''
                CREATE INDEX IF NOT EXISTS idx_timestamp 
                ON context_entries (timestamp)
            ''')
            
            conn.execute('''
                CREATE INDEX IF NOT EXISTS idx_importance 
                ON context_entries (importance_score)
            ''')
            
            conn.execute('''
                CREATE TABLE IF NOT EXISTS context_relationships (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    from_entry_id TEXT NOT NULL,
                    to_entry_id TEXT NOT NULL,
                    relationship_type TEXT NOT NULL,
                    strength REAL NOT NULL,
                    created_at TEXT NOT NULL,
                    FOREIGN KEY (from_entry_id) REFERENCES context_entries (entry_id),
                    FOREIGN KEY (to_entry_id) REFERENCES context_entries (entry_id)
                )
            ''')
    
    def _setup_logging(self) -> logging.Logger:
        """Î°úÍπÖ ÏÑ§Ï†ï"""
        logger = logging.getLogger('context_persistence')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.FileHandler(self.context_dir / 'context.log')
            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def _initialize_processing_rules(self) -> Dict[str, Any]:
        """Ïª®ÌÖçÏä§Ìä∏ Ï≤òÎ¶¨ Í∑úÏπô Ï¥àÍ∏∞Ìôî"""
        return {
            'importance_calculation': {
                ContextType.DECISION_HISTORY: 0.9,
                ContextType.ERROR_PATTERN: 0.8,
                ContextType.SUCCESS_PATTERN: 0.8,
                ContextType.LEARNING_OUTCOME: 0.7,
                ContextType.TASK_EXECUTION: 0.6,
                ContextType.COMMUNICATION: 0.5,
                ContextType.WORKFLOW_STATE: 0.4,
                ContextType.KNOWLEDGE_BASE: 0.3
            },
            'retention_periods': {
                ContextType.DECISION_HISTORY: timedelta(days=365),
                ContextType.ERROR_PATTERN: timedelta(days=180),
                ContextType.SUCCESS_PATTERN: timedelta(days=180),
                ContextType.LEARNING_OUTCOME: timedelta(days=90),
                ContextType.TASK_EXECUTION: timedelta(days=30),
                ContextType.COMMUNICATION: timedelta(days=30),
                ContextType.WORKFLOW_STATE: timedelta(days=7),
                ContextType.KNOWLEDGE_BASE: None  # ÏòÅÍµ¨ Î≥¥Ï°¥
            },
            'auto_tagging_keywords': {
                'critical_decision': ['critical', 'important', 'decision', 'choose', 'select'],
                'error_handling': ['error', 'exception', 'failure', 'bug', 'issue'],
                'performance': ['performance', 'speed', 'optimization', 'efficiency'],
                'quality': ['quality', 'review', 'validation', 'testing', 'standard']
            }
        }
    
    def store_context(self, 
                     role_id: str,
                     context_type: ContextType,
                     content: Dict[str, Any],
                     context_scope: ContextScope = ContextScope.ROLE_SPECIFIC,
                     project_id: Optional[str] = None,
                     custom_tags: List[str] = None,
                     importance_override: Optional[float] = None) -> str:
        """Ïª®ÌÖçÏä§Ìä∏ Ï†ÄÏû•"""
        
        # ÏóîÌä∏Î¶¨ ÏÉùÏÑ±
        entry_id = self._generate_entry_id(role_id, context_type)
        timestamp = datetime.now()
        
        # Ï§ëÏöîÎèÑ Í≥ÑÏÇ∞
        importance_score = importance_override or self._calculate_importance(context_type, content)
        
        # ÏûêÎèô ÌÉúÍπÖ
        auto_tags = self._auto_tag_content(content)
        tags = list(set((custom_tags or []) + auto_tags))
        
        # Î≥¥Ï°¥ Í∏∞Í∞Ñ ÏÑ§Ï†ï
        retention_period = self.processing_rules['retention_periods'].get(context_type)
        
        # Í¥ÄÎ†® ÏóîÌä∏Î¶¨ Ï∞æÍ∏∞
        related_entries = self._find_related_entries(role_id, context_type, content)
        
        context_entry = ContextEntry(
            entry_id=entry_id,
            context_type=context_type,
            context_scope=context_scope,
            role_id=role_id,
            project_id=project_id,
            timestamp=timestamp,
            content=content,
            metadata={
                'source': 'system_generated',
                'processing_version': '1.0',
                'content_size': len(str(content))
            },
            tags=tags,
            importance_score=importance_score,
            retention_period=retention_period,
            related_entries=related_entries
        )
        
        # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ï†ÄÏû•
        self._save_to_database(context_entry)
        
        # Î©îÎ™®Î¶¨ Ï∫êÏãú ÏóÖÎç∞Ïù¥Ìä∏
        with self.cache_lock:
            self.memory_cache[entry_id] = context_entry
        
        # Í¥ÄÍ≥Ñ Ï†ÄÏû•
        self._store_relationships(context_entry)
        
        self.logger.info(f"Context stored: {entry_id} for role {role_id}")
        return entry_id
    
    def retrieve_context(self, query: ContextQuery) -> List[ContextEntry]:
        """Ïª®ÌÖçÏä§Ìä∏ Ï°∞Ìöå"""
        
        # SQL ÏøºÎ¶¨ Íµ¨ÏÑ±
        sql = "SELECT * FROM context_entries WHERE 1=1"
        params = []
        
        if query.role_id:
            sql += " AND role_id = ?"
            params.append(query.role_id)
        
        if query.context_types:
            type_placeholders = ','.join(['?' for _ in query.context_types])
            sql += f" AND context_type IN ({type_placeholders})"
            params.extend([ct.value for ct in query.context_types])
        
        if query.time_range:
            sql += " AND timestamp BETWEEN ? AND ?"
            params.extend([query.time_range[0].isoformat(), query.time_range[1].isoformat()])
        
        if query.importance_threshold:
            sql += " AND importance_score >= ?"
            params.append(query.importance_threshold)
        
        if query.project_id:
            sql += " AND project_id = ?"
            params.append(query.project_id)
        
        # ÌÉúÍ∑∏ ÌïÑÌÑ∞ÎßÅ
        if query.tags:
            for tag in query.tags:
                sql += " AND tags_json LIKE ?"
                params.append(f'%"{tag}"%')
        
        # ÌÇ§ÏõåÎìú ÌïÑÌÑ∞ÎßÅ
        if query.content_keywords:
            for keyword in query.content_keywords:
                sql += " AND content_json LIKE ?"
                params.append(f'%{keyword}%')
        
        # Ï†ïÎ†¨ Î∞è Ï†úÌïú
        sql += " ORDER BY importance_score DESC, timestamp DESC"
        
        if query.limit:
            sql += " LIMIT ?"
            params.append(query.limit)
        
        # ÏøºÎ¶¨ Ïã§Ìñâ
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.execute(sql, params)
            rows = cursor.fetchall()
        
        # ContextEntry Í∞ùÏ≤¥Î°ú Î≥ÄÌôò
        entries = []
        for row in rows:
            entry = self._row_to_context_entry(row)
            entries.append(entry)
        
        self.logger.info(f"Retrieved {len(entries)} context entries")
        return entries
    
    def get_role_context_summary(self, role_id: str, days_back: int = 30) -> Dict[str, Any]:
        """Ïó≠Ìï†Î≥Ñ Ïª®ÌÖçÏä§Ìä∏ ÏöîÏïΩ"""
        
        # ÏµúÍ∑º Ïª®ÌÖçÏä§Ìä∏ Ï°∞Ìöå
        time_range = (datetime.now() - timedelta(days=days_back), datetime.now())
        query = ContextQuery(
            role_id=role_id,
            time_range=time_range
        )
        
        entries = self.retrieve_context(query)
        
        # ÏöîÏïΩ ÏÉùÏÑ±
        summary = {
            'role_id': role_id,
            'period': f"ÏµúÍ∑º {days_back}Ïùº",
            'total_entries': len(entries),
            'by_type': {},
            'key_decisions': [],
            'learned_patterns': [],
            'frequent_errors': [],
            'success_factors': [],
            'communication_patterns': {},
            'performance_trends': [],
            'recommendations': []
        }
        
        # ÌÉÄÏûÖÎ≥Ñ Î∂ÑÎ•ò
        for entry in entries:
            entry_type = entry.context_type.value
            if entry_type not in summary['by_type']:
                summary['by_type'][entry_type] = 0
            summary['by_type'][entry_type] += 1
        
        # Ï§ëÏöîÌïú ÏùòÏÇ¨Í≤∞Ï†ï Ï∂îÏ∂ú
        decision_entries = [e for e in entries if e.context_type == ContextType.DECISION_HISTORY]
        decision_entries.sort(key=lambda x: x.importance_score, reverse=True)
        for entry in decision_entries[:5]:
            summary['key_decisions'].append({
                'timestamp': entry.timestamp.isoformat(),
                'decision': entry.content.get('decision_summary', 'Unknown'),
                'importance': entry.importance_score
            })
        
        # ÌïôÏäµ Ìå®ÌÑ¥ Ï∂îÏ∂ú
        learning_entries = [e for e in entries if e.context_type == ContextType.LEARNING_OUTCOME]
        for entry in learning_entries[-3:]:  # ÏµúÍ∑º 3Í∞ú
            summary['learned_patterns'].append({
                'timestamp': entry.timestamp.isoformat(),
                'pattern': entry.content.get('pattern_description', 'Unknown'),
                'confidence': entry.content.get('confidence_score', 0.0)
            })
        
        # Ïò§Î•ò Ìå®ÌÑ¥ Î∂ÑÏÑù
        error_entries = [e for e in entries if e.context_type == ContextType.ERROR_PATTERN]
        error_counts = {}
        for entry in error_entries:
            error_type = entry.content.get('error_type', 'unknown')
            error_counts[error_type] = error_counts.get(error_type, 0) + 1
        
        summary['frequent_errors'] = [
            {'error_type': k, 'count': v} 
            for k, v in sorted(error_counts.items(), key=lambda x: x[1], reverse=True)[:3]
        ]
        
        # ÏÑ±Í≥µ ÏöîÏù∏ Î∂ÑÏÑù
        success_entries = [e for e in entries if e.context_type == ContextType.SUCCESS_PATTERN]
        for entry in success_entries[-3:]:
            summary['success_factors'].append({
                'timestamp': entry.timestamp.isoformat(),
                'factor': entry.content.get('success_factor', 'Unknown'),
                'impact': entry.content.get('impact_score', 0.0)
            })
        
        # Í∂åÍ≥†ÏÇ¨Ìï≠ ÏÉùÏÑ±
        summary['recommendations'] = self._generate_recommendations(role_id, entries)
        
        return summary
    
    def store_decision_history(self, 
                             role_id: str,
                             decision_context: Dict[str, Any],
                             decision_made: str,
                             alternatives_considered: List[str],
                             decision_rationale: str,
                             confidence_level: float,
                             project_id: Optional[str] = None) -> str:
        """ÏùòÏÇ¨Í≤∞Ï†ï Ïù¥Î†• Ï†ÄÏû•"""
        
        content = {
            'decision_context': decision_context,
            'decision_made': decision_made,
            'alternatives_considered': alternatives_considered,
            'decision_rationale': decision_rationale,
            'confidence_level': confidence_level,
            'decision_summary': f"{decision_made} (Ïã†Î¢∞ÎèÑ: {confidence_level:.2f})"
        }
        
        return self.store_context(
            role_id=role_id,
            context_type=ContextType.DECISION_HISTORY,
            content=content,
            project_id=project_id,
            custom_tags=['decision', 'critical'] if confidence_level > 0.8 else ['decision'],
            importance_override=0.9 if confidence_level > 0.8 else None
        )
    
    def store_task_execution(self,
                           role_id: str,
                           task_description: str,
                           execution_steps: List[str],
                           execution_result: Dict[str, Any],
                           performance_metrics: Dict[str, Any],
                           project_id: Optional[str] = None) -> str:
        """ÏûëÏóÖ Ïã§Ìñâ Í∏∞Î°ù Ï†ÄÏû•"""
        
        content = {
            'task_description': task_description,
            'execution_steps': execution_steps,
            'execution_result': execution_result,
            'performance_metrics': performance_metrics,
            'success': execution_result.get('success', False),
            'execution_time': performance_metrics.get('execution_time', 0),
            'quality_score': execution_result.get('quality_score', 0.0)
        }
        
        tags = ['task_execution']
        if content['success']:
            tags.append('successful')
        else:
            tags.append('failed')
        
        return self.store_context(
            role_id=role_id,
            context_type=ContextType.TASK_EXECUTION,
            content=content,
            project_id=project_id,
            custom_tags=tags
        )
    
    def store_learning_outcome(self,
                             role_id: str,
                             learning_context: str,
                             pattern_discovered: str,
                             pattern_details: Dict[str, Any],
                             confidence_score: float,
                             applicable_scenarios: List[str],
                             project_id: Optional[str] = None) -> str:
        """ÌïôÏäµ Í≤∞Í≥º Ï†ÄÏû•"""
        
        content = {
            'learning_context': learning_context,
            'pattern_discovered': pattern_discovered,
            'pattern_details': pattern_details,
            'confidence_score': confidence_score,
            'applicable_scenarios': applicable_scenarios,
            'pattern_description': f"{pattern_discovered} (Ïã†Î¢∞ÎèÑ: {confidence_score:.2f})"
        }
        
        return self.store_context(
            role_id=role_id,
            context_type=ContextType.LEARNING_OUTCOME,
            content=content,
            project_id=project_id,
            custom_tags=['learning', 'pattern', 'intelligence']
        )
    
    def store_error_pattern(self,
                          role_id: str,
                          error_type: str,
                          error_details: Dict[str, Any],
                          occurrence_frequency: int,
                          resolution_steps: List[str],
                          prevention_strategies: List[str],
                          project_id: Optional[str] = None) -> str:
        """Ïò§Î•ò Ìå®ÌÑ¥ Ï†ÄÏû•"""
        
        content = {
            'error_type': error_type,
            'error_details': error_details,
            'occurrence_frequency': occurrence_frequency,
            'resolution_steps': resolution_steps,
            'prevention_strategies': prevention_strategies,
            'severity': error_details.get('severity', 'medium')
        }
        
        tags = ['error', 'pattern']
        if content['severity'] == 'high':
            tags.append('critical')
        
        return self.store_context(
            role_id=role_id,
            context_type=ContextType.ERROR_PATTERN,
            content=content,
            project_id=project_id,
            custom_tags=tags,
            importance_override=0.9 if content['severity'] == 'high' else None
        )
    
    def get_relevant_context_for_task(self, 
                                    role_id: str,
                                    current_task_description: str,
                                    max_entries: int = 10) -> List[ContextEntry]:
        """ÌòÑÏû¨ ÏûëÏóÖÏóê Í¥ÄÎ†®Îêú Ïª®ÌÖçÏä§Ìä∏ Ï°∞Ìöå"""
        
        # ÌÇ§ÏõåÎìú Ï∂îÏ∂ú
        keywords = self._extract_keywords(current_task_description)
        
        # Í¥ÄÎ†® Ïª®ÌÖçÏä§Ìä∏ ÏøºÎ¶¨
        query = ContextQuery(
            role_id=role_id,
            content_keywords=keywords[:5],  # ÏÉÅÏúÑ 5Í∞ú ÌÇ§ÏõåÎìú
            importance_threshold=0.3,
            limit=max_entries * 2  # ÌïÑÌÑ∞ÎßÅÏùÑ ÏúÑÌï¥ Îçî ÎßéÏù¥ Ï°∞Ìöå
        )
        
        entries = self.retrieve_context(query)
        
        # Í¥ÄÎ†®ÏÑ± Ï†êÏàò Í≥ÑÏÇ∞ Î∞è Ï†ïÎ†¨
        scored_entries = []
        for entry in entries:
            relevance_score = self._calculate_relevance_score(
                current_task_description, entry
            )
            scored_entries.append((entry, relevance_score))
        
        # Í¥ÄÎ†®ÏÑ± Ï†êÏàò Í∏∞Ï§Ä Ï†ïÎ†¨
        scored_entries.sort(key=lambda x: x[1], reverse=True)
        
        # ÏÉÅÏúÑ Ìï≠Î™© Î∞òÌôò
        return [entry for entry, score in scored_entries[:max_entries]]
    
    # Helper methods
    def _generate_entry_id(self, role_id: str, context_type: ContextType) -> str:
        """ÏóîÌä∏Î¶¨ ID ÏÉùÏÑ±"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')
        return f"{role_id}_{context_type.value}_{timestamp}"
    
    def _calculate_importance(self, context_type: ContextType, content: Dict[str, Any]) -> float:
        """Ï§ëÏöîÎèÑ Í≥ÑÏÇ∞"""
        base_importance = self.processing_rules['importance_calculation'].get(context_type, 0.5)
        
        # Ïª®ÌÖêÏ∏† Í∏∞Î∞ò Ï°∞Ï†ï
        adjustments = 0.0
        
        # ÏÑ±Í≥µ/Ïã§Ìå®Ïóê Îî∞Î•∏ Ï°∞Ï†ï
        if content.get('success') is False:
            adjustments += 0.1  # Ïã§Ìå®Îäî ÌïôÏäµ Í∏∞Ìöå
        elif content.get('success') is True:
            adjustments += 0.05  # ÏÑ±Í≥µÎèÑ Ï§ëÏöî
        
        # Ïã†Î¢∞ÎèÑ/ÌíàÏßà Ï†êÏàòÏóê Îî∞Î•∏ Ï°∞Ï†ï
        confidence = content.get('confidence_level') or content.get('confidence_score', 0.5)
        if confidence > 0.8:
            adjustments += 0.1
        
        quality_score = content.get('quality_score', 0.5)
        if quality_score > 0.8:
            adjustments += 0.05
        
        return min(1.0, base_importance + adjustments)
    
    def _auto_tag_content(self, content: Dict[str, Any]) -> List[str]:
        """Ïª®ÌÖêÏ∏† ÏûêÎèô ÌÉúÍπÖ"""
        tags = []
        content_text = str(content).lower()
        
        for tag, keywords in self.processing_rules['auto_tagging_keywords'].items():
            if any(keyword in content_text for keyword in keywords):
                tags.append(tag)
        
        return tags
    
    def _find_related_entries(self, 
                            role_id: str, 
                            context_type: ContextType, 
                            content: Dict[str, Any]) -> List[str]:
        """Í¥ÄÎ†® ÏóîÌä∏Î¶¨ Ï∞æÍ∏∞"""
        # Í∞ÑÎã®Ìïú Íµ¨ÌòÑ - Ïã§Ï†úÎ°úÎäî Îçî Ï†ïÍµêÌïú Ïú†ÏÇ¨ÏÑ± Î∂ÑÏÑù ÌïÑÏöî
        keywords = self._extract_keywords(str(content))[:3]
        
        if not keywords:
            return []
        
        query = ContextQuery(
            role_id=role_id,
            content_keywords=keywords,
            limit=5
        )
        
        related_entries = self.retrieve_context(query)
        return [entry.entry_id for entry in related_entries[-5:]]  # ÏµúÍ∑º 5Í∞ú
    
    def _extract_keywords(self, text: str) -> List[str]:
        """ÌÇ§ÏõåÎìú Ï∂îÏ∂ú"""
        # Í∞ÑÎã®Ìïú ÌÇ§ÏõåÎìú Ï∂îÏ∂ú - Ïã§Ï†úÎ°úÎäî NLP ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÇ¨Ïö© Í∂åÏû•
        words = text.lower().split()
        
        # Î∂àÏö©Ïñ¥ Ï†úÍ±∞
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
        keywords = [word for word in words if len(word) > 3 and word not in stop_words]
        
        # ÎπàÎèÑ Í∏∞Î∞ò Ï†ïÎ†¨ (Í∞ÑÎã®Ìïú Íµ¨ÌòÑ)
        from collections import Counter
        word_counts = Counter(keywords)
        
        return [word for word, count in word_counts.most_common(10)]
    
    def _calculate_relevance_score(self, task_description: str, entry: ContextEntry) -> float:
        """Í¥ÄÎ†®ÏÑ± Ï†êÏàò Í≥ÑÏÇ∞"""
        task_keywords = set(self._extract_keywords(task_description))
        entry_keywords = set(self._extract_keywords(str(entry.content)))
        
        # ÌÇ§ÏõåÎìú ÍµêÏßëÌï© Í∏∞Î∞ò Í¥ÄÎ†®ÏÑ±
        if not task_keywords or not entry_keywords:
            return 0.0
        
        intersection = len(task_keywords & entry_keywords)
        union = len(task_keywords | entry_keywords)
        
        keyword_similarity = intersection / union if union > 0 else 0.0
        
        # Ï§ëÏöîÎèÑÏôÄ ÏãúÍ∞Ñ Í∞ÄÏ§ëÏπò Ï†ÅÏö©
        time_weight = max(0.1, 1.0 - (datetime.now() - entry.timestamp).days / 365)
        
        return keyword_similarity * entry.importance_score * time_weight
    
    def _save_to_database(self, entry: ContextEntry):
        """Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Ïóê Ï†ÄÏû•"""
        content_hash = hashlib.md5(str(entry.content).encode()).hexdigest()
        
        with sqlite3.connect(self.db_path) as conn:
            conn.execute('''
                INSERT OR REPLACE INTO context_entries 
                (entry_id, context_type, context_scope, role_id, project_id, timestamp,
                 content_json, metadata_json, tags_json, importance_score, 
                 retention_period, related_entries_json, content_hash, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                entry.entry_id,
                entry.context_type.value,
                entry.context_scope.value,
                entry.role_id,
                entry.project_id,
                entry.timestamp.isoformat(),
                json.dumps(entry.content, ensure_ascii=False),
                json.dumps(entry.metadata, ensure_ascii=False),
                json.dumps(entry.tags, ensure_ascii=False),
                entry.importance_score,
                entry.retention_period.total_seconds() if entry.retention_period else None,
                json.dumps(entry.related_entries, ensure_ascii=False),
                content_hash,
                datetime.now().isoformat()
            ))
    
    def _store_relationships(self, entry: ContextEntry):
        """Í¥ÄÍ≥Ñ Ï†ÄÏû•"""
        if not entry.related_entries:
            return
        
        with sqlite3.connect(self.db_path) as conn:
            for related_id in entry.related_entries:
                conn.execute('''
                    INSERT INTO context_relationships 
                    (from_entry_id, to_entry_id, relationship_type, strength, created_at)
                    VALUES (?, ?, ?, ?, ?)
                ''', (
                    entry.entry_id,
                    related_id,
                    'content_similarity',
                    0.7,  # Í∏∞Î≥∏ Í∞ïÎèÑ
                    datetime.now().isoformat()
                ))
    
    def _row_to_context_entry(self, row: sqlite3.Row) -> ContextEntry:
        """Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÌñâÏùÑ ContextEntryÎ°ú Î≥ÄÌôò"""
        return ContextEntry(
            entry_id=row['entry_id'],
            context_type=ContextType(row['context_type']),
            context_scope=ContextScope(row['context_scope']),
            role_id=row['role_id'],
            project_id=row['project_id'],
            timestamp=datetime.fromisoformat(row['timestamp']),
            content=json.loads(row['content_json']),
            metadata=json.loads(row['metadata_json']),
            tags=json.loads(row['tags_json']),
            importance_score=row['importance_score'],
            retention_period=timedelta(seconds=row['retention_period']) if row['retention_period'] else None,
            related_entries=json.loads(row['related_entries_json'])
        )
    
    def _generate_recommendations(self, role_id: str, entries: List[ContextEntry]) -> List[str]:
        """Í∂åÍ≥†ÏÇ¨Ìï≠ ÏÉùÏÑ±"""
        recommendations = []
        
        # Ïò§Î•ò Ìå®ÌÑ¥ Í∏∞Î∞ò Í∂åÍ≥†
        error_entries = [e for e in entries if e.context_type == ContextType.ERROR_PATTERN]
        if len(error_entries) > 3:
            recommendations.append("Î∞òÎ≥µÎêòÎäî Ïò§Î•ò Ìå®ÌÑ¥Ïù¥ Î∞úÍ≤¨ÎêòÏóàÏäµÎãàÎã§. ÏòàÎ∞© Ï†ÑÎûµÏùÑ Í∞ïÌôîÌïòÏÑ∏Ïöî.")
        
        # ÏÑ±Í≥µ Ìå®ÌÑ¥ Í∏∞Î∞ò Í∂åÍ≥†
        success_entries = [e for e in entries if e.context_type == ContextType.SUCCESS_PATTERN]
        if success_entries:
            recommendations.append("ÏÑ±Í≥µÏ†ÅÏù∏ Ìå®ÌÑ¥ÏùÑ Îã§Î•∏ ÏûëÏóÖÏóêÎèÑ Ï†ÅÏö©Ìï¥Î≥¥ÏÑ∏Ïöî.")
        
        # ÏùòÏÇ¨Í≤∞Ï†ï Ìå®ÌÑ¥ Î∂ÑÏÑù
        decision_entries = [e for e in entries if e.context_type == ContextType.DECISION_HISTORY]
        low_confidence_decisions = [e for e in decision_entries if e.content.get('confidence_level', 1.0) < 0.6]
        if len(low_confidence_decisions) > 2:
            recommendations.append("Ïã†Î¢∞ÎèÑÍ∞Ä ÎÇÆÏùÄ ÏùòÏÇ¨Í≤∞Ï†ïÏù¥ ÎßéÏäµÎãàÎã§. Îçî ÎßéÏùÄ Ï†ïÎ≥¥ ÏàòÏßëÏùÑ Í≥†Î†§ÌïòÏÑ∏Ïöî.")
        
        return recommendations

def main():
    """ÌÖåÏä§Ìä∏ Î∞è Îç∞Î™®"""
    context_system = ContextPersistenceSystem("/home/jungh/workspace/multi_claude_code_sample")
    
    # ÏòàÏãú 1: ÏùòÏÇ¨Í≤∞Ï†ï Ïù¥Î†• Ï†ÄÏû•
    decision_id = context_system.store_decision_history(
        role_id="business_analyst",
        decision_context={
            "situation": "API Î≤ÑÏ†Ñ ÏÑ†ÌÉù",
            "constraints": ["ÏÑ±Îä• ÏöîÍµ¨ÏÇ¨Ìï≠", "Ìò∏ÌôòÏÑ±", "Î≥¥Ïïà"],
            "stakeholders": ["Í∞úÎ∞úÌåÄ", "Ïö¥ÏòÅÌåÄ"]
        },
        decision_made="REST API v2.0 ÏÇ¨Ïö©",
        alternatives_considered=["GraphQL", "REST API v1.0", "gRPC"],
        decision_rationale="ÏÑ±Îä•Í≥º Ìò∏ÌôòÏÑ±Ïùò Í∑†ÌòïÏù¥ Í∞ÄÏû• Ï¢ãÏùå",
        confidence_level=0.85,
        project_id="web_community"
    )
    print(f"‚úÖ ÏùòÏÇ¨Í≤∞Ï†ï Ïù¥Î†• Ï†ÄÏû•: {decision_id}")
    
    # ÏòàÏãú 2: ÏûëÏóÖ Ïã§Ìñâ Í∏∞Î°ù Ï†ÄÏû•
    task_id = context_system.store_task_execution(
        role_id="frontend_developer",
        task_description="ÏÇ¨Ïö©Ïûê Î°úÍ∑∏Ïù∏ Ìèº Íµ¨ÌòÑ",
        execution_steps=[
            "ÏöîÍµ¨ÏÇ¨Ìï≠ Î∂ÑÏÑù",
            "UI Ïª¥Ìè¨ÎÑåÌä∏ ÏÑ§Í≥Ñ",
            "Ìèº Í≤ÄÏ¶ù Î°úÏßÅ Íµ¨ÌòÑ",
            "API ÌÜµÌï©",
            "ÌÖåÏä§Ìä∏ ÏàòÌñâ"
        ],
        execution_result={
            "success": True,
            "deliverable_path": "/src/components/LoginForm.tsx",
            "quality_score": 0.92
        },
        performance_metrics={
            "execution_time": 240,  # Î∂Ñ
            "code_coverage": 0.85,
            "bugs_found": 1
        },
        project_id="web_community"
    )
    print(f"‚úÖ ÏûëÏóÖ Ïã§Ìñâ Í∏∞Î°ù Ï†ÄÏû•: {task_id}")
    
    # ÏòàÏãú 3: ÌïôÏäµ Í≤∞Í≥º Ï†ÄÏû•
    learning_id = context_system.store_learning_outcome(
        role_id="qa_tester",
        learning_context="ÏûêÎèôÌôî ÌÖåÏä§Ìä∏ Ïã§Ìñâ Ï§ë Î∞úÍ≤¨Îêú Ìå®ÌÑ¥",
        pattern_discovered="API ÏùëÎãµ ÏãúÍ∞ÑÏù¥ DB ÏøºÎ¶¨ Î≥µÏû°ÎèÑÏôÄ Í∞ïÌïú ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ",
        pattern_details={
            "correlation_coefficient": 0.87,
            "sample_size": 150,
            "statistical_significance": 0.001
        },
        confidence_score=0.9,
        applicable_scenarios=["ÏÑ±Îä• ÌÖåÏä§Ìä∏", "API ÏµúÏ†ÅÌôî", "Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÌäúÎãù"],
        project_id="web_community"
    )
    print(f"‚úÖ ÌïôÏäµ Í≤∞Í≥º Ï†ÄÏû•: {learning_id}")
    
    # ÏòàÏãú 4: Ïó≠Ìï†Î≥Ñ Ïª®ÌÖçÏä§Ìä∏ ÏöîÏïΩ Ï°∞Ìöå
    summary = context_system.get_role_context_summary("business_analyst", days_back=7)
    print("üìä Business Analyst Ïª®ÌÖçÏä§Ìä∏ ÏöîÏïΩ:")
    print(f"  - Ï¥ù ÏóîÌä∏Î¶¨: {summary['total_entries']}")
    print(f"  - Ï£ºÏöî ÏùòÏÇ¨Í≤∞Ï†ï: {len(summary['key_decisions'])}Í∞ú")
    print(f"  - Í∂åÍ≥†ÏÇ¨Ìï≠: {len(summary['recommendations'])}Í∞ú")
    
    # ÏòàÏãú 5: ÌòÑÏû¨ ÏûëÏóÖ Í¥ÄÎ†® Ïª®ÌÖçÏä§Ìä∏ Ï°∞Ìöå
    relevant_context = context_system.get_relevant_context_for_task(
        role_id="frontend_developer",
        current_task_description="ÏÇ¨Ïö©Ïûê ÎåÄÏãúÎ≥¥Îìú Ïª¥Ìè¨ÎÑåÌä∏ Íµ¨ÌòÑ",
        max_entries=5
    )
    print(f"üîç Í¥ÄÎ†® Ïª®ÌÖçÏä§Ìä∏ {len(relevant_context)}Í∞ú Î∞úÍ≤¨")

if __name__ == "__main__":
    main()